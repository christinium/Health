{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/christinium/Health/blob/main/EchoProject/Gemma_prompt_only_echo_label_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYVYKCxvRmMS"
   },
   "source": [
    "## Using Gemma to Label Echo Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mha6gF9R60Oc",
    "outputId": "155d1723-4328-4316-a92e-a8a351b34a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up working directory\n",
    "import os\n",
    "WORKING_DIR = '/content/drive/MyDrive/echo_training/'  # Change this to your preferred location\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "os.chdir(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFJ919YX648F"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PVCtnuD63g4"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeDb5i7J9QHB"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('echo_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvO2Q_iPiiMF"
   },
   "outputs": [],
   "source": [
    "\n",
    "test_df = test_df.rename(columns={test_df.columns[0]: 'id_num'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6Ayx6xIYU2r"
   },
   "outputs": [],
   "source": [
    "# Constants and configurations\n",
    "LABEL_NAMES = [\n",
    "    'LA_cavity', 'RA_dilated', 'LV_systolic', 'LV_cavity',\n",
    "    'LV_wall', 'RV_cavity', 'RV_systolic', 'AV_stenosis',\n",
    "    'MV_stenosis', 'TV_regurgitation', 'TV_stenosis',\n",
    "    'TV_pulm_htn', 'AV_regurgitation', 'MV_regurgitation',\n",
    "    'RA_pressure', 'LV_diastolic', 'RV_volume_overload',\n",
    "    'RV_wall', 'RV_pressure_overload'\n",
    "]\n",
    "\n",
    "# Medical context for each feature\n",
    "FEATURE_CONTEXT = {\n",
    "    'LA_cavity': 'left atrial cavity size',\n",
    "    'RA_dilated': 'right atrial dilation',\n",
    "    'LV_systolic': 'left ventricular systolic function',\n",
    "    'LV_cavity': 'left ventricular cavity size',\n",
    "    'LV_wall': 'left ventricular wall size',\n",
    "    'RV_cavity': 'right ventricular cavity size',\n",
    "    'RV_systolic': 'right ventricular systolic function',\n",
    "    'AV_stenosis': 'atrial virus stenoses',\n",
    "    'MV_stenosis': 'mitral valve stenoses',\n",
    "    'TV_regurgitation': 'tricuspid valve regurgitation',\n",
    "    'TV_stenosis': 'tricuspid valve stenoses',\n",
    "    'TV_pulm_htn': 'tricuspid valve pulmonary hypertension',\n",
    "    'AV_regurgitation': 'atrial virus regurgitation',\n",
    "    'MV_regurgitation': 'mitral valve regurgitation',\n",
    "    'RA_pressure': 'right atrial pressure',\n",
    "    'LV_diastolic': 'left ventricular diastolic function',\n",
    "    'RV_volume_overload': 'right ventricular volume overload',\n",
    "    'RV_wall': 'right ventricular wall thickness',\n",
    "    'RV_pressure_overload': 'right ventricular pressure overload'\n",
    "}\n",
    "\n",
    "CODING_SCHEMA = {\n",
    "    \"Study is not adequate to evaluate\": -3,\n",
    "    \"Abnormality not present\": 0,\n",
    "    \"Abnormality is present but not quantifiable\": -2,\n",
    "    \"Abnormality can be categorized as\": {\n",
    "        \"hyperdynamic\": -1,\n",
    "        \"normal\": 0,\n",
    "        \"mild dysfunction\": 1,\n",
    "        \"moderate dysfunction\": 2,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnHgOHzhYU5b"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROMPT ENGINEERING: Create structured prompts for Gemma model\n",
    "# ============================================================================\n",
    "\n",
    "def create_structured_prompt(text: str) -> str:\n",
    "    \"\"\"Creates a structured medical prompt for echo analysis.\"\"\"\n",
    "\n",
    "    # Format feature context\n",
    "    feature_list = \"\\n\".join([f\"- {key}: {value}\" for key, value in FEATURE_CONTEXT.items()])\n",
    "\n",
    "    # Format expected output\n",
    "    expected_output = \"\\n\".join([f\"{label}: [number]\" for label in LABEL_NAMES])\n",
    "\n",
    "    return f\"\"\"<start_of_turn>user\n",
    "As a cardiologist, analyze this echocardiogram report and provide a structured assessment.\n",
    "\n",
    "For each feature:\n",
    "{feature_list}\n",
    "\n",
    "Use this coding schema to evaluate:\n",
    "-3: Study is not adequate to evaluate\n",
    " 0: Abnormality not present\n",
    "-2: Abnormality is present but not quantifiable\n",
    "-1: Hyperdynamic\n",
    " 0: Normal\n",
    " 1: Mild dysfunction\n",
    " 2: Moderate dysfunction\n",
    " 3: Severe dysfunction\n",
    "\n",
    "Format your response exactly as follows:\n",
    "{expected_output}\n",
    "\n",
    "Report:\n",
    "{text}<end_of_turn>\n",
    "<start_of_turn>model\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-Mxukt6F2hS"
   },
   "outputs": [],
   "source": [
    "test_df['gemma_long_prompt'] = test_df['text'].apply(create_structured_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRq6fcUkcnzg",
    "outputId": "726d4847-ef60-46db-ba62-17bff5bf131e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "As a cardiologist, analyze this echocardiogram report and provide a structured assessment. \n",
      "\n",
      "For each feature:\n",
      "- LA_cavity: left atrial cavity size\n",
      "- RA_dilated: right atrial dilation\n",
      "- LV_systolic: left ventricular systolic function\n",
      "- LV_cavity: left ventricular cavity size\n",
      "- LV_wall: left ventricular wall size\n",
      "- RV_cavity: right ventricular cavity size\n",
      "- RV_systolic: right ventricular systolic function\n",
      "- AV_stenosis: atrial virus stenoses\n",
      "- MV_stenosis: mitral valve stenoses\n",
      "- TV_regurgitation: tricuspid valve regurgitation\n",
      "- TV_stenosis: tricuspid valve stenoses\n",
      "- TV_pulm_htn: tricuspid valve pulmonary hypertension\n",
      "- AV_regurgitation: atrial virus regurgitation\n",
      "- MV_regurgitation: mitral valve regurgitation\n",
      "- RA_pressure: right atrial pressure\n",
      "- LV_diastolic: left ventricular diastolic function\n",
      "- RV_volume_overload: right ventricular volume overload\n",
      "- RV_wall: right ventricular wall thickness\n",
      "- RV_pressure_overload: right ventricular pressure overload\n",
      "\n",
      "Use this coding schema to evaluate:\n",
      "-3: Study is not adequate to evaluate\n",
      " 0: Abnormality not present\n",
      "-2: Abnormality is present but not quantifiable\n",
      "-1: Hyperdynamic\n",
      " 0: Normal\n",
      " 1: Mild dysfunction\n",
      " 2: Moderate dysfunction\n",
      " 3: Severe dysfunction\n",
      "\n",
      "Format your response exactly as follows:\n",
      "LA_cavity: [number]\n",
      "RA_dilated: [number]\n",
      "LV_systolic: [number]\n",
      "LV_cavity: [number]\n",
      "LV_wall: [number]\n",
      "RV_cavity: [number]\n",
      "RV_systolic: [number]\n",
      "AV_stenosis: [number]\n",
      "MV_stenosis: [number]\n",
      "TV_regurgitation: [number]\n",
      "TV_stenosis: [number]\n",
      "TV_pulm_htn: [number]\n",
      "AV_regurgitation: [number]\n",
      "MV_regurgitation: [number]\n",
      "RA_pressure: [number]\n",
      "LV_diastolic: [number]\n",
      "RV_volume_overload: [number]\n",
      "RV_wall: [number]\n",
      "RV_pressure_overload: [number]\n",
      "\n",
      "Report:\n",
      "LEFT ATRIUM: The left atrium is normal in size.\n",
      "\n",
      "RIGHT ATRIUM/INTERATRIAL SEPTUM: The right atrium is normal in size.\n",
      "\n",
      "RIGHT VENTRICLE: Right ventricular chamber size and free wall motion are\n",
      "normal.\n",
      "\n",
      "PERICARDIUM: There is no pericardial effusion.<end_of_turn>\n",
      "<start_of_turn>model\n"
     ]
    }
   ],
   "source": [
    "# Here is a sample of what the prompt looks like\n",
    "print(test_df['prompt'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460,
     "referenced_widgets": [
      "85336c55c394489593633cc6d48734d6",
      "673c956a7cf5469c84fd413d2dc73f5a",
      "fca62385fa5349a99770b739173e8c0f",
      "b08a891fdb2449eaaa342b74fde77c06",
      "49d8bc074081403ea7e536c42bd21bfc",
      "788577a5cb024a6ca4125fc712e81123",
      "05581a28660d4cadbfc0500f72df9b30",
      "2f1cd1b913904d27ace9b4947b5f68bf",
      "8dceef329a3e4d0b9ea7b0721d9083c0",
      "423dfe458c8a4849a09b6140b5c279a9",
      "6dd84400c8f74352b2d3bf2a5ab8be5e",
      "187ccfaad3004922851ba7139d7e0fb9",
      "3391d68ef5fb46f09ec447307fde2de3",
      "4cbfd71355614f158901443898f0f97d",
      "0b00390c2d0d4f198b70e040a09605a5",
      "c0b9420e1ae949728b9ae1ca7dfb9eb1",
      "785da26cfd6048b684c313a3b88e3c37",
      "04e2e6d9398a42468330f588dc1fe82f",
      "c06f1b247d76498595e1e706167fe6bd",
      "1d3ee65c1603425593c10564c23f91b5",
      "f12ed52ae9914c25b6ef9b058556e0ba",
      "89c2c5aada3a4daf920397cb13c28f26",
      "ac3a83fac66140c4bf7f8272e082bf08",
      "653b189e1e1f4469999c3685e62a4a70",
      "644ae5dd555d4e95a7674ae74aa93a6e",
      "5a1727ab36084cac93beeeef96fd27df",
      "ee12bae1d1d44224b95ece1aef82f3ca",
      "fd6e5536e9ec4837934136b9671954c4",
      "6f0d28dd751d4919ba4dc0d8b3f1173d",
      "abb52578992c4385b59da48aef358537",
      "52b2b7a36c024591ab7a3746db120999",
      "7b7b0bef59184f7b8c6136dbc673f333",
      "2dd2711af5384eb6a4790e387c971c67",
      "59ea9a893e054f1b9d3b5f839b6c0af9",
      "1ad1665d55434002ae230dc830ffa89f",
      "750b24a09d7e44f8aa82921081b61744",
      "272542e5c0c240f58740245dec83e0bb",
      "3c2bcfefa14a4bfc8f0930ff785ffb6d",
      "7cf7f392dfc4450ab016357bc300d727",
      "7c91052d4e4844c784cccfb53b4ca56f",
      "cc9f354810874f589b56d29d50a11e1b",
      "6f3449996a324d19ae3ad549986680b2",
      "f6a9a87c3ae84ed8aa366c6f216d3140",
      "c5927164445746c9a568ba07d06bcc90",
      "dccdd3d494be409c8062a790dec234dc",
      "788e68569814496595c1e4146462392a",
      "85d952d38d1f48f792b6729dc0b21bea",
      "315384abaf014ba09b27fbfcd5149ac4",
      "0cf91f404fc4457d8c0d453d7459df8d",
      "f096f43c95874b339aefb0e4d16eb1e1",
      "96bf9580a52d4d358bec204bcb0598fd",
      "7dbd9eb4e22b438396d87784fdabe9e2",
      "9eba8c4d04794868b136fdb88e20a218",
      "e1fecde9fcb54bdbb5b45df74cf6ad47",
      "bb2afc1ffc69447b895495ae5a5e9405",
      "61cc24c1efb1479e91ee249a0301f4db",
      "dd0adce4e098494f9e6c3787c3b73d9b",
      "d8ae44457a84422282a384a95f1f3e03",
      "c329b1c2e6824b7f93bd6f548b92ca38",
      "ebf2e0a39000466cab1c31426e0d914c",
      "a538455b36fa4d389e23462e132b151c",
      "36108305b5e34bdc92d944cc64e7b471",
      "71477662c73548ecaff81e68342c6619",
      "798200212f474ec6ac41e466914a13fd",
      "073fb12df29f4d6fa49f85843cdf3a02",
      "f2c832aeef4f4ebfabaab3cfd216bc66",
      "01528d7b90e54f3ea6e7a67f059bd73d",
      "3760fdbed44541b9a92ba3643e5d7203",
      "d0523667a48e43dfa1621a3c8d8b7eb7",
      "cfd06b75b7c341a8b6a5b0843b28c95f",
      "084cf0ff46c84c848ac173c32073c5ca",
      "60b7213808ea48b58eb1beea4d39eefa",
      "b44f3d2f80ed483db0747f737f65486b",
      "85e927f3694f4027953a652151305f63",
      "61b463a38eec4a9d8d8e38b96fe369d4",
      "0c3e9c3b62d0468c999236b593a2fd2a",
      "6a1f2fc44f5040bcbc1f4206247c87b2",
      "d7262cfd7c6140a8bb3fc119c5e16681",
      "c71cf7c8a8014103a43d56b3a4e3d58e",
      "b46466d104534c7fa9360c9c7b17b316",
      "3bcd7f406ff0458ba165af0d0a38d35c",
      "e4c90f0329b64ea5b2b739743fce1b63",
      "ba588341dcb14bf59ab84728fc9a42d9",
      "917da0d4bcd444ac8233a026f591822d",
      "888808b4ca024ad286da53d87fae6051",
      "91c1945b13c9418c85c7d727a261a3fe",
      "99f77aa909e846258870f951690c59e4",
      "344de05fc81941838cef786af4b6a076",
      "68657df654604ed89f9f9bfcac8d8ff2",
      "6a7fbc0b18f54a6dbed035ffe4dc22ff",
      "c48ed037e9b9488f9661da6f3a7d2086",
      "584b0065ae8841479b9529e652bef207",
      "35d3db6c766d47ceb8a928bc9401c766",
      "32d5b2ce55ae41a2a9aed1f40f89be67",
      "67ca24691f2c42afb4f85c367da991e4",
      "4db0b0518db345c6bf92304ed491f68d",
      "8865b1b4c5d2423a92543dd2246bf1a2",
      "21e50671a7f442bca3d855c97f54c6ab",
      "da08657d515049949ec41bfb597352ed",
      "7c60f7f95ef84f4fb1c208ba95725ca2",
      "7b7ff023a4e44c0c8dfdab0f3cbd6382",
      "cff485c5a84a4ab3b07ba5d739c5e008",
      "2aa616d931144ed1873643a686c86cb0",
      "b75bea8158a7403697a76ad67035b8fe",
      "d8d0d2b928c1461bb3bfe21bc4cc2bc3",
      "5e2f5f5b2cd34f2bbba0f96a2a1c6840",
      "ff7f2cae420744adae9a92b20e0b326f",
      "bd048f551fb74f5db7377cf6048edafc",
      "a1a1591bdd304d26ad49ca0d2c136404",
      "1f2f977cb54442a7aad027400b62e69b",
      "fc4d17535b0e4558adff9cf548cbb43d",
      "f946f6438600429995b7da8dc8b5ce6d",
      "d913e8d13c1e4b6e965cd8a91236b20b",
      "92e37a5db0d345818e214732ce57000c",
      "9af80882f8134742ab86ae5b9eb58ad8",
      "1b7a38e8be9940e3be278baf3cab7bea",
      "a2f4470e30614f1b9c85a4cbff6130a0",
      "ec6af1befd6e4b90aa4080361eff530b",
      "050392bbb16c4936a52dea8a1777623b",
      "e0ab4784521049b2ae51bfd2ccad4bfd",
      "9762d11b5b254426852af5efd6cb13f2"
     ]
    },
    "id": "5QpZoGwibqQZ",
    "outputId": "cb24ef2f-63df-4cce-f3b0-c2ca992f1456"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85336c55c394489593633cc6d48734d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187ccfaad3004922851ba7139d7e0fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3a83fac66140c4bf7f8272e082bf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ea9a893e054f1b9d3b5f839b6c0af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccdd3d494be409c8062a790dec234dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cc24c1efb1479e91ee249a0301f4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01528d7b90e54f3ea6e7a67f059bd73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7262cfd7c6140a8bb3fc119c5e16681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68657df654604ed89f9f9bfcac8d8ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c60f7f95ef84f4fb1c208ba95725ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4d17535b0e4558adff9cf548cbb43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL LOADING: Initialize Gemma 2B instruction-tuned model\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Gemma model\n",
    "model_name = 'google/gemma-2b-it' # 2B parameter instruction-tuned variant\n",
    "# Load tokenizer (converts text to tokens the model understands)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Load model with optimizations:\n",
    "# - torch_dtype=torch.bfloat16: Use bfloat16 precision (saves memory, faster inference)\n",
    "# - device_map=\"auto\": Automatically distribute model across available GPUs\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljGbZnbJjwjy"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BATCH INFERENCE: Run model predictions in batches for efficiency\n",
    "# ============================================================================\n",
    "\n",
    "def run_gemma_batch(prompts_list, batch_size=4):\n",
    "    \"\"\"Run multiple prompts in a batch.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(prompts_list), batch_size):\n",
    "        batch = prompts_list[i:i+batch_size]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode each output in the batch\n",
    "        for j, output in enumerate(outputs):\n",
    "            response = tokenizer.decode(\n",
    "                output[inputs['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            results.append(response)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44eeqO1ejwmT",
    "outputId": "f9ba9bc6-1b0f-46ec-8cd5-11d1c241af7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [28:43<00:00, 16.57s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# RUN INFERENCE: Process all echo reports through Gemma model\n",
    "# ============================================================================\n",
    "# Process in batches\n",
    "batch_size = 64   # Adjust based on your GPU memory (64 works on Colab high-RAM)\n",
    "prompts_list = test_df['prompt'].tolist()\n",
    "results = []\n",
    "\n",
    "# Run inference with progress bar\n",
    "for i in tqdm(range(0, len(prompts_list), batch_size)):\n",
    "    batch = prompts_list[i:i+batch_size]\n",
    "    batch_results = run_gemma_batch(batch, batch_size=batch_size)\n",
    "    results.extend(batch_results)\n",
    "# Store raw model outputs\n",
    "test_df['gemma_result'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CN9NDBveFh0B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKdvHBYcx6LT"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Save predictions to avoid re-running inference\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f'test_df_with_long_gemma_predictions_{timestamp}.csv'\n",
    "test_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1U1GQDt7uwY"
   },
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv('/content/drive/MyDrive/echo_training/test_df_with_gemma_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BvktRn_48VP2"
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.rename(columns={temp_df.columns[0]: 'id_num'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6nxukIOlwUl0",
    "outputId": "2afb2b0b-0776-4e49-be99-b2b87c262e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GEMMA MODEL PERFORMANCE ON ECHO NOTES\n",
      "================================================================================\n",
      "\n",
      "Overall Accuracy: 0.2024 (20.24%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Per-Label Performance:\n",
      "--------------------------------------------------------------------------------\n",
      "               Label  Accuracy  Correct  Total  Percentage\n",
      "           LA_cavity  0.158398      530   3346   15.839809\n",
      "          RA_dilated  0.191320      626   3272   19.132029\n",
      "         LV_systolic  0.063887      213   3334    6.388722\n",
      "           LV_cavity  0.139409      462   3314   13.940857\n",
      "             LV_wall  0.253628      839   3308   25.362757\n",
      "           RV_cavity  0.219824      723   3289   21.982365\n",
      "         RV_systolic  0.253180      836   3302   25.317989\n",
      "         AV_stenosis  0.093809      300   3198    9.380863\n",
      "         MV_stenosis  0.190709      624   3272   19.070905\n",
      "    TV_regurgitation  0.200738      653   3253   20.073778\n",
      "         TV_stenosis  0.250787      797   3178   25.078666\n",
      "         TV_pulm_htn  0.249529      795   3186   24.952919\n",
      "    AV_regurgitation  0.281039      876   3117   28.103946\n",
      "    MV_regurgitation  0.329416     1037   3148   32.941550\n",
      "         RA_pressure  0.210797      656   3112   21.079692\n",
      "        LV_diastolic  0.186473      579   3105   18.647343\n",
      "  RV_volume_overload  0.223743      703   3142   22.374284\n",
      "             RV_wall  0.150689      481   3192   15.068922\n",
      "RV_pressure_overload  0.209479      663   3165   20.947867\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PERFORMANCE EVALUATION: Parse model outputs and calculate accuracy\n",
    "# ============================================================================\n",
    "\n",
    "# Load your results\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/echo_training/test_df_with_gemma_predictions.csv')\n",
    "test_df = test_df.rename(columns={test_df.columns[0]: 'id_num'})\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 1: Parse raw model outputs into structured predictions\n",
    "# We are using regular expressions because the Gemma output is\n",
    "# not consistent.\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def parse_medical_report(report_text):\n",
    "    \"\"\"\n",
    "    Extract label:value pairs from model's text output.\n",
    "\n",
    "    Example input: \"LA_cavity: 0\\nRA_dilated: 1\\n...\"\n",
    "    Example output: {'LA_cavity': 0, 'RA_dilated': 1, ...}\n",
    "    \"\"\"\n",
    "    pattern = r'\\*?\\*?([A-Z_a-z]+)\\s*:\\s*(\\d+)\\*?\\*?'\n",
    "    matches = re.findall(pattern, str(report_text))\n",
    "    parsed_data = {}\n",
    "    for key, value in matches:\n",
    "        if key in LABEL_NAMES:\n",
    "            parsed_data[key] = int(value)\n",
    "    return parsed_data\n",
    "\n",
    "def parse_to_list(parsed_data):\n",
    "    \"\"\"Convert parsed dictionary to ordered list matching LABEL_NAMES.\"\"\"\n",
    "    return [parsed_data.get(key, None) for key in LABEL_NAMES]\n",
    "\n",
    "# Process predictions: Convert text outputs to numeric lists\n",
    "test_df['gemma_result_list'] = test_df['gemma_result'].apply(\n",
    "    lambda x: parse_to_list(parse_medical_report(str(x))) if pd.notna(x) else []\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 2: Calculate accuracy metrics\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def calculate_accuracy(df):\n",
    "    \"\"\"\n",
    "    Compare model predictions against ground truth labels.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with overall accuracy and per-label breakdown\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'overall_accuracy': 0,\n",
    "        'per_label_accuracy': {},\n",
    "        'per_label_correct': {},\n",
    "        'per_label_total': {}\n",
    "    }\n",
    "\n",
    "    correct_per_label = {label: 0 for label in LABEL_NAMES}\n",
    "    total_per_label = {label: 0 for label in LABEL_NAMES}\n",
    "    total_predictions = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        labels = row['labels_parsed']  # Ground truth\n",
    "        predictions = row['gemma_result_list']  # Model predictions\n",
    "\n",
    "        # Parse labels if stored as string\n",
    "        if isinstance(labels, str):\n",
    "            try:\n",
    "                labels = ast.literal_eval(labels)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Skip invalid rows\n",
    "        if labels is None or predictions is None or len(labels) == 0 or len(predictions) == 0:\n",
    "            continue\n",
    "\n",
    "        # Compare each label position\n",
    "        for i, label_name in enumerate(LABEL_NAMES):\n",
    "            if i >= min(len(labels), len(predictions)):\n",
    "                break\n",
    "\n",
    "            label_val = labels[i]\n",
    "            pred_val = predictions[i]\n",
    "\n",
    "            if label_val is None or pred_val is None:\n",
    "                continue\n",
    "\n",
    "            # Normalize to integers for comparison\n",
    "            try:\n",
    "                label_val = int(float(label_val))\n",
    "                pred_val = int(float(pred_val))\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "\n",
    "            total_per_label[label_name] += 1\n",
    "            total_predictions += 1\n",
    "\n",
    "            # Check if prediction matches ground truth\n",
    "            if label_val == pred_val:\n",
    "                correct_per_label[label_name] += 1\n",
    "                total_correct += 1\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    if total_predictions > 0:\n",
    "        results['overall_accuracy'] = total_correct / total_predictions\n",
    "\n",
    "    # Calculate per-label accuracy\n",
    "    for label in LABEL_NAMES:\n",
    "        if total_per_label[label] > 0:\n",
    "            results['per_label_accuracy'][label] = correct_per_label[label] / total_per_label[label]\n",
    "            results['per_label_correct'][label] = correct_per_label[label]\n",
    "            results['per_label_total'][label] = total_per_label[label]\n",
    "        else:\n",
    "            results['per_label_accuracy'][label] = None\n",
    "            results['per_label_correct'][label] = 0\n",
    "            results['per_label_total'][label] = 0\n",
    "\n",
    "    return results\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# STEP 3: Display results\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Run accuracy calculation\n",
    "accuracy_results = calculate_accuracy(test_df)\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 80)\n",
    "print(\"GEMMA MODEL PERFORMANCE ON ECHO NOTES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOverall Accuracy: {accuracy_results['overall_accuracy']:.4f} ({accuracy_results['overall_accuracy']*100:.2f}%)\")\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Per-Label Performance:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'Label': LABEL_NAMES,\n",
    "    'Accuracy': [accuracy_results['per_label_accuracy'][label] if accuracy_results['per_label_accuracy'][label] is not None else 0\n",
    "                 for label in LABEL_NAMES],\n",
    "    'Correct': [accuracy_results['per_label_correct'][label] for label in LABEL_NAMES],\n",
    "    'Total': [accuracy_results['per_label_total'][label] for label in LABEL_NAMES]\n",
    "})\n",
    "summary_df['Percentage'] = summary_df['Accuracy'] * 100\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO2i5os0p3A09PPWK8Jl7mh",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
