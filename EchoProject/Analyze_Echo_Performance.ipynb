{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7zpJ0SAyRXS"
   },
   "source": [
    "# Echo Results Review\n",
    "\n",
    "This notebook evaluates the performance of a fine-tuned language model on an echocardiogram report analysis task using a test dataset. It loads the model, defines the target labels, runs inference on each test example to generate predictions, and then calculates and reports the accuracy of the model's predictions against the true labels for each feature, as well as the overall exact match accuracy.\n",
    "\n",
    "The model used was created by echo_note_training_final.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWe6XAZ-xpBN",
    "outputId": "d5fddaea-1f4e-4cd5-b113-8fc7d50e3190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up working directory\n",
    "import os\n",
    "WORKING_DIR = '/content/drive/MyDrive/echo_training/'  # Change this to your preferred location\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "os.chdir(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yvGlzCufyfLA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HEU6Eg7JzBo2"
   },
   "outputs": [],
   "source": [
    "# Loads the split files created when we trained the model.\n",
    "#train_df = pd.read_csv('echo_train.csv')\n",
    "#tune_df = pd.read_csv('echo_tune.csv')\n",
    "test_df = pd.read_csv('echo_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Pyp9X_O5zDiu"
   },
   "outputs": [],
   "source": [
    "# Making a copy of the test dataframe - so I don't accidentally modify it\n",
    "test_df_copy = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F5lpo5BGzgCE"
   },
   "outputs": [],
   "source": [
    "test_df_copy = test_df_copy.rename(columns={test_df_copy.columns[0]: 'id_num'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "c972bd7fb03841a9a8e70452b1d72573",
      "61224c0ff65d40418aace1d9faa92f5f",
      "7df87cd072b7488395dedf8619f496e2",
      "86969bdb37d64a6094345b40f2d9ee48",
      "de814ac5831f4dd0a86093cae20dad46",
      "8564d4a469134ac896bb5b4df2a3ce96",
      "3eb9561fd1df49c8b3020452f3df6247",
      "8094262be72a48a7a3725aa815540fac",
      "41d0f2c0c9b040a39a54a61d9e9ad468",
      "d18c25278b2c4e4f848e124503e32cd7",
      "6b471e3cfc0b46b9b5158b3d0953fb08"
     ]
    },
    "id": "iwDVLfmcyiif",
    "outputId": "4f89e0c1-76eb-4e02-f6c8-70c60f74802e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c972bd7fb03841a9a8e70452b1d72573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# SETUP: LOAD MODEL AND DEFINE LABELS\n",
    "# ==============================================================================\n",
    "\n",
    "# Define label names\n",
    "LABEL_NAMES = [\n",
    "    'LA_cavity', 'RA_dilated', 'LV_systolic', 'LV_cavity',\n",
    "    'LV_wall', 'RV_cavity', 'RV_systolic', 'AV_stenosis',\n",
    "    'MV_stenosis', 'TV_regurgitation', 'TV_stenosis',\n",
    "    'TV_pulm_htn', 'AV_regurgitation', 'MV_regurgitation',\n",
    "    'RA_pressure', 'LV_diastolic', 'RV_volume_overload',\n",
    "    'RV_wall', 'RV_pressure_overload'\n",
    "]\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"final_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zAp7BJopykxw"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# INFERENCE FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_prediction(text):\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "Analyze this echocardiogram report and provide assessment values for each cardiac feature. Output should be in the format \"feature: value\" for each of the 19 features.\n",
    "\n",
    "Report:\n",
    "{text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.1,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the model's response\n",
    "    model_output = full_output.split(\"<start_of_turn>model\\n\")[-1].strip()\n",
    "    return model_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mnxe2a5nynCH"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PARSE PREDICTIONS - SIMPLER VERSION\n",
    "# ==============================================================================\n",
    "\n",
    "def parse_prediction(pred_text):\n",
    "    \"\"\"Extract predicted label values from model output text\"\"\"\n",
    "    predicted = []\n",
    "    lines = pred_text.split('\\n')\n",
    "\n",
    "    for label_name in LABEL_NAMES:\n",
    "        found = False\n",
    "        for line in lines:\n",
    "            if label_name in line and ':' in line:\n",
    "                try:\n",
    "                    # Get text after colon, remove spaces, convert to int\n",
    "                    value_str = line.split(':')[1].strip()\n",
    "                    value = int(value_str)\n",
    "                    predicted.append(value)\n",
    "                    found = True\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        if not found:\n",
    "            predicted.append(None)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ptIkFmaGACbs"
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# BATCH INFERENCE FUNCTION - OPTIMIZED VERSION\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_predictions_batch(texts, batch_size=16): #you can decrease the batch size if you have memory constraints\n",
    "    \"\"\"\n",
    "    Generate predictions for a batch of texts at once.\n",
    "\n",
    "    Args:\n",
    "        texts: List of echo text strings to process\n",
    "        batch_size: Number of texts to process together (adjust based on GPU memory)\n",
    "\n",
    "    Returns:\n",
    "        List of prediction strings\n",
    "    \"\"\"\n",
    "    # Create prompts for all texts in batch\n",
    "    prompts = []\n",
    "    for text in texts:\n",
    "        prompt = f\"\"\"<start_of_turn>user\n",
    "Analyze this echocardiogram report and provide assessment values for each cardiac feature. Output should be in the format \"feature: value\" for each of the 19 features.\n",
    "\n",
    "Report:\n",
    "{text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # Tokenize all prompts at once with padding\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048  # Adjust if your reports are longer\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate for entire batch\n",
    "    with torch.no_grad():  # Saves memory\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode all outputs\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "        full_output = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        # Extract just the model's response\n",
    "        model_output = full_output.split(\"<start_of_turn>model\\n\")[-1].strip()\n",
    "        predictions.append(model_output)\n",
    "\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77UmcBdZA66x",
    "outputId": "286cece1-51dc-40f2-a312-6f8020603c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BATCH inference on 6608 test examples...\n",
      "Batch size: 16\n",
      "Estimated time: ~0.8 hours (vs 13+ hours single)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 413/413 [1:12:35<00:00, 10.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved results to test_inference_results.csv\n",
      "Shape: (6608, 5)\n",
      "\n",
      "ðŸš€ Batch processing complete! This was much faster than single-item processing!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# RUN BATCH INFERENCE ON ALL TEST EXAMPLES\n",
    "# ==============================================================================\n",
    "\n",
    "results = []\n",
    "batch_size = 16  # Start with 8, increase to 16 or 32 if you have GPU memory available\n",
    "\n",
    "print(f\"Running BATCH inference on {len(test_df_copy)} test examples...\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Estimated time: ~{(len(test_df_copy) / batch_size) * 7.27 / 3600:.1f} hours (vs 13+ hours single)\")\n",
    "\n",
    "# Process in batches\n",
    "for start_idx in tqdm(range(0, len(test_df_copy), batch_size)):\n",
    "    end_idx = min(start_idx + batch_size, len(test_df_copy))\n",
    "    batch_df = test_df_copy.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Get batch data\n",
    "    batch_texts = batch_df['text'].tolist()\n",
    "    batch_ids = batch_df['id_num'].tolist()\n",
    "    batch_true_labels = batch_df['labels'].tolist()\n",
    "\n",
    "    # Generate predictions for entire batch at once\n",
    "    batch_predictions = generate_predictions_batch(batch_texts, batch_size)\n",
    "\n",
    "    # Store results for each item in batch\n",
    "    for i, (idx, text, true_labels_raw, id_num, pred_text) in enumerate(\n",
    "        zip(range(start_idx, end_idx), batch_texts, batch_true_labels, batch_ids, batch_predictions)\n",
    "    ):\n",
    "        # Parse true labels\n",
    "        if isinstance(true_labels_raw, str):\n",
    "            true_labels = ast.literal_eval(true_labels_raw)\n",
    "        else:\n",
    "            true_labels = true_labels_raw\n",
    "\n",
    "        # Store result\n",
    "        result = {\n",
    "            'idx': idx,\n",
    "            'id_num': id_num,\n",
    "            'echo_text': text,\n",
    "            'true_labels': true_labels,\n",
    "            'prediction_text': pred_text\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('/content/drive/MyDrive/echo_training/test_inference_results_batch_run.csv', index=False)\n",
    "print(f\"\\nSaved results to test_inference_results_batch_run.csv\")\n",
    "print(f\"Shape: {results_df.shape}\")\n",
    "print(f\"\\nðŸš€ Batch processing complete! This was much faster than single-item processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DN10KKyytoo",
    "outputId": "f386235f-8b21-4bdc-8076-73ee45443ab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LABEL DISTRIBUTION IN TEST SET\n",
      "======================================================================\n",
      "\n",
      "LA_cavity:\n",
      "  -50:    91 (  1.4%)\n",
      "   -3:    10 (  0.2%)\n",
      "   -2:   486 (  7.4%)\n",
      "    0:  4379 ( 66.3%)\n",
      "    1:  1076 ( 16.3%)\n",
      "    2:   565 (  8.6%)\n",
      "    3:     1 (  0.0%)\n",
      "\n",
      "RA_dilated:\n",
      "    0:  4563 ( 69.1%)\n",
      "    1:  2045 ( 30.9%)\n",
      "\n",
      "LV_systolic:\n",
      "  -50:    30 (  0.5%)\n",
      "   -3:    38 (  0.6%)\n",
      "   -2:   123 (  1.9%)\n",
      "   -1:   262 (  4.0%)\n",
      "    0:  4719 ( 71.4%)\n",
      "    1:   452 (  6.8%)\n",
      "    2:   385 (  5.8%)\n",
      "    3:   599 (  9.1%)\n",
      "\n",
      "LV_cavity:\n",
      "  -50:     8 (  0.1%)\n",
      "   -3:    28 (  0.4%)\n",
      "   -2:    31 (  0.5%)\n",
      "   -1:   138 (  2.1%)\n",
      "    0:  5806 ( 87.9%)\n",
      "    1:   232 (  3.5%)\n",
      "    2:   292 (  4.4%)\n",
      "    3:    73 (  1.1%)\n",
      "\n",
      "LV_wall:\n",
      "  -50:    11 (  0.2%)\n",
      "   -3:    26 (  0.4%)\n",
      "   -2:    30 (  0.5%)\n",
      "    0:  4418 ( 66.9%)\n",
      "    1:  1768 ( 26.8%)\n",
      "    2:   280 (  4.2%)\n",
      "    3:    75 (  1.1%)\n",
      "\n",
      "RV_cavity:\n",
      "  -50:    41 (  0.6%)\n",
      "   -3:   189 (  2.9%)\n",
      "   -2:   311 (  4.7%)\n",
      "   -1:    23 (  0.3%)\n",
      "    0:  5240 ( 79.3%)\n",
      "    1:   491 (  7.4%)\n",
      "    2:   313 (  4.7%)\n",
      "\n",
      "RV_systolic:\n",
      "  -50:    48 (  0.7%)\n",
      "   -3:   212 (  3.2%)\n",
      "   -2:   375 (  5.7%)\n",
      "    0:  5266 ( 79.7%)\n",
      "    1:   341 (  5.2%)\n",
      "    2:   199 (  3.0%)\n",
      "    3:   167 (  2.5%)\n",
      "\n",
      "AV_stenosis:\n",
      "  -50:     2 (  0.0%)\n",
      "   -3:    20 (  0.3%)\n",
      "   -2:    30 (  0.5%)\n",
      "    0:  5792 ( 87.7%)\n",
      "    1:   339 (  5.1%)\n",
      "    2:   129 (  2.0%)\n",
      "    3:   296 (  4.5%)\n",
      "\n",
      "MV_stenosis:\n",
      "  -50:     2 (  0.0%)\n",
      "   -3:     7 (  0.1%)\n",
      "    0:  6510 ( 98.5%)\n",
      "    1:    61 (  0.9%)\n",
      "    2:    20 (  0.3%)\n",
      "    3:     8 (  0.1%)\n",
      "\n",
      "TV_regurgitation:\n",
      "  -50:    64 (  1.0%)\n",
      "   -3:   171 (  2.6%)\n",
      "   -2:    54 (  0.8%)\n",
      "    0:  6316 ( 95.6%)\n",
      "    2:     2 (  0.0%)\n",
      "    3:     1 (  0.0%)\n",
      "\n",
      "TV_stenosis:\n",
      "  -50:     7 (  0.1%)\n",
      "   -3:   226 (  3.4%)\n",
      "    0:  6373 ( 96.4%)\n",
      "    1:     2 (  0.0%)\n",
      "\n",
      "TV_pulm_htn:\n",
      "  -50:    59 (  0.9%)\n",
      "   -3:   781 ( 11.8%)\n",
      "    0:  3377 ( 51.1%)\n",
      "    1:  1312 ( 19.9%)\n",
      "    2:   947 ( 14.3%)\n",
      "    3:   132 (  2.0%)\n",
      "\n",
      "AV_regurgitation:\n",
      "  -50:    25 (  0.4%)\n",
      "   -3:     1 (  0.0%)\n",
      "   -2:    35 (  0.5%)\n",
      "    0:  4461 ( 67.5%)\n",
      "    1:  1904 ( 28.8%)\n",
      "    2:   115 (  1.7%)\n",
      "    3:    67 (  1.0%)\n",
      "\n",
      "MV_regurgitation:\n",
      "  -50:   170 (  2.6%)\n",
      "   -2:    79 (  1.2%)\n",
      "    0:  3894 ( 58.9%)\n",
      "    1:  1666 ( 25.2%)\n",
      "    2:   497 (  7.5%)\n",
      "    3:   302 (  4.6%)\n",
      "\n",
      "RA_pressure:\n",
      "   -3:    73 (  1.1%)\n",
      "    0:  6251 ( 94.6%)\n",
      "    1:   207 (  3.1%)\n",
      "    2:    77 (  1.2%)\n",
      "\n",
      "LV_diastolic:\n",
      "  -50:     7 (  0.1%)\n",
      "   -3:    61 (  0.9%)\n",
      "   -2:   263 (  4.0%)\n",
      "    0:  6163 ( 93.3%)\n",
      "    1:    61 (  0.9%)\n",
      "    2:    31 (  0.5%)\n",
      "    3:    22 (  0.3%)\n",
      "\n",
      "RV_volume_overload:\n",
      "  -50:     1 (  0.0%)\n",
      "   -3:   221 (  3.3%)\n",
      "    0:  6201 ( 93.8%)\n",
      "    1:   185 (  2.8%)\n",
      "\n",
      "RV_wall:\n",
      "  -50:     4 (  0.1%)\n",
      "   -3:   221 (  3.3%)\n",
      "    0:  6220 ( 94.1%)\n",
      "    1:   163 (  2.5%)\n",
      "\n",
      "RV_pressure_overload:\n",
      "  -50:     1 (  0.0%)\n",
      "   -3:   217 (  3.3%)\n",
      "    0:  6168 ( 93.3%)\n",
      "    1:   222 (  3.4%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# PART 1: LABEL DISTRIBUTION IN TEST SET\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LABEL DISTRIBUTION IN TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, label_name in enumerate(LABEL_NAMES):\n",
    "    print(f\"\\n{label_name}:\")\n",
    "\n",
    "    # Extract the i-th value from each label list\n",
    "    label_values = []\n",
    "    for idx in range(len(test_df_copy)):\n",
    "        labels_raw = test_df_copy.iloc[idx]['labels']\n",
    "\n",
    "        # Parse if string\n",
    "        if isinstance(labels_raw, str):\n",
    "            labels = ast.literal_eval(labels_raw)\n",
    "        else:\n",
    "            labels = labels_raw\n",
    "\n",
    "        label_values.append(labels[i])\n",
    "\n",
    "    # Count values\n",
    "    value_counts = pd.Series(label_values).value_counts().sort_index()\n",
    "    null_count = pd.Series(label_values).isna().sum()\n",
    "    total = len(label_values)\n",
    "\n",
    "    for value, count in value_counts.items():\n",
    "        pct = (count/total)*100\n",
    "        print(f\"  {value:>3}: {count:>5} ({pct:>5.1f}%)\")\n",
    "    if null_count > 0:\n",
    "        pct = (null_count/total)*100\n",
    "        print(f\"  Null: {null_count:>5} ({pct:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Um2SYHO8ywQf",
    "outputId": "9b3089a4-de69-410f-e7c8-5148bdc83b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Parsing predictions...\n",
      "\n",
      "======================================================================\n",
      "ACCURACY BY LABEL\n",
      "======================================================================\n",
      "\n",
      "LA_cavity:\n",
      "  Correct: 6596/6607 = 0.998\n",
      "  Unparseable: 1\n",
      "\n",
      "RA_dilated:\n",
      "  Correct: 6607/6607 = 1.000\n",
      "  Unparseable: 1\n",
      "\n",
      "LV_systolic:\n",
      "  Correct: 6598/6607 = 0.999\n",
      "  Unparseable: 1\n",
      "\n",
      "LV_cavity:\n",
      "  Correct: 6604/6607 = 1.000\n",
      "  Unparseable: 1\n",
      "\n",
      "LV_wall:\n",
      "  Correct: 6597/6607 = 0.998\n",
      "  Unparseable: 1\n",
      "\n",
      "RV_cavity:\n",
      "  Correct: 6605/6607 = 1.000\n",
      "  Unparseable: 1\n",
      "\n",
      "RV_systolic:\n",
      "  Correct: 6601/6607 = 0.999\n",
      "  Unparseable: 1\n",
      "\n",
      "AV_stenosis:\n",
      "  Correct: 6599/6607 = 0.999\n",
      "  Unparseable: 1\n",
      "\n",
      "MV_stenosis:\n",
      "  Correct: 6602/6607 = 0.999\n",
      "  Unparseable: 1\n",
      "\n",
      "TV_regurgitation:\n",
      "  Correct: 6602/6607 = 0.999\n",
      "  Unparseable: 1\n",
      "\n",
      "TV_stenosis:\n",
      "  Correct: 6606/6607 = 1.000\n",
      "  Unparseable: 1\n",
      "\n",
      "TV_pulm_htn:\n",
      "  Correct: 6603/6607 = 0.999\n",
      "  Unparseable: 1\n",
      "\n",
      "AV_regurgitation:\n",
      "  Correct: 6579/6607 = 0.996\n",
      "  Unparseable: 1\n",
      "\n",
      "MV_regurgitation:\n",
      "  Correct: 6581/6607 = 0.996\n",
      "  Unparseable: 1\n",
      "\n",
      "RA_pressure:\n",
      "  Correct: 6607/6607 = 1.000\n",
      "  Unparseable: 1\n",
      "\n",
      "LV_diastolic:\n",
      "  Correct: 6598/6607 = 0.999\n",
      "  Unparseable: 1\n",
      "\n",
      "RV_volume_overload:\n",
      "  Correct: 6605/6607 = 1.000\n",
      "  Unparseable: 1\n",
      "\n",
      "RV_wall:\n",
      "  Correct: 6606/6607 = 1.000\n",
      "  Unparseable: 1\n",
      "\n",
      "RV_pressure_overload:\n",
      "  Correct: 6604/6607 = 1.000\n",
      "  Unparseable: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# PART 2: CALCULATE ACCURACY\n",
    "# ==============================================================================\n",
    "\n",
    "# Parse all predictions\n",
    "print(\"\\n\\nParsing predictions...\")\n",
    "results_df['pred_labels'] = results_df['prediction_text'].apply(parse_prediction)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACCURACY BY LABEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "accuracy_results = []\n",
    "\n",
    "for i, label_name in enumerate(LABEL_NAMES):\n",
    "    # Extract true values (i-th element from true_labels list)\n",
    "    true_vals = results_df['true_labels'].apply(lambda x: x[i] if i < len(x) else None).values\n",
    "\n",
    "    # Extract predicted values\n",
    "    pred_vals = results_df['pred_labels'].apply(lambda x: x[i] if x and i < len(x) else None).values\n",
    "\n",
    "    # Remove any None predictions\n",
    "    valid_mask = ~pd.isna(pred_vals)\n",
    "    true_vals_valid = true_vals[valid_mask]\n",
    "    pred_vals_valid = pred_vals[valid_mask]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = (true_vals_valid == pred_vals_valid).sum()\n",
    "    total = len(true_vals_valid)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    # Count unparseable predictions\n",
    "    unparseable = (~valid_mask).sum()\n",
    "\n",
    "    accuracy_results.append({\n",
    "        'label': label_name,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'accuracy': accuracy,\n",
    "        'unparseable': unparseable\n",
    "    })\n",
    "\n",
    "    print(f\"\\n{label_name}:\")\n",
    "    print(f\"  Correct: {correct}/{total} = {accuracy:.3f}\")\n",
    "    if unparseable > 0:\n",
    "        print(f\"  Unparseable: {unparseable}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ns08RtFyymJ",
    "outputId": "23375bd0-8bb6-4120-8cc3-db7a7a2bbdc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXACT MATCH (all 19 labels correct): 6481/6608 = 0.981\n",
      "======================================================================\n",
      "\n",
      "Accuracy results saved to label_accuracy.csv\n",
      "\n",
      "ACCURACY SUMMARY:\n",
      "               label  correct  total  accuracy  unparseable\n",
      "           LA_cavity     6596   6607  0.998335            1\n",
      "          RA_dilated     6607   6607  1.000000            1\n",
      "         LV_systolic     6598   6607  0.998638            1\n",
      "           LV_cavity     6604   6607  0.999546            1\n",
      "             LV_wall     6597   6607  0.998486            1\n",
      "           RV_cavity     6605   6607  0.999697            1\n",
      "         RV_systolic     6601   6607  0.999092            1\n",
      "         AV_stenosis     6599   6607  0.998789            1\n",
      "         MV_stenosis     6602   6607  0.999243            1\n",
      "    TV_regurgitation     6602   6607  0.999243            1\n",
      "         TV_stenosis     6606   6607  0.999849            1\n",
      "         TV_pulm_htn     6603   6607  0.999395            1\n",
      "    AV_regurgitation     6579   6607  0.995762            1\n",
      "    MV_regurgitation     6581   6607  0.996065            1\n",
      "         RA_pressure     6607   6607  1.000000            1\n",
      "        LV_diastolic     6598   6607  0.998638            1\n",
      "  RV_volume_overload     6605   6607  0.999697            1\n",
      "             RV_wall     6606   6607  0.999849            1\n",
      "RV_pressure_overload     6604   6607  0.999546            1\n"
     ]
    }
   ],
   "source": [
    "# Create accuracy summary DataFrame\n",
    "accuracy_df = pd.DataFrame(accuracy_results)\n",
    "\n",
    "# Overall exact match accuracy\n",
    "exact_matches = sum(1 for idx in range(len(results_df))\n",
    "                    if results_df.iloc[idx]['true_labels'] == results_df.iloc[idx]['pred_labels'])\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"EXACT MATCH (all 19 labels correct): {exact_matches}/{len(results_df)} = {exact_matches/len(results_df):.3f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save accuracy results\n",
    "accuracy_df.to_csv('label_accuracy.csv', index=False)\n",
    "print(\"\\nAccuracy results saved to label_accuracy.csv\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nACCURACY SUMMARY:\")\n",
    "print(accuracy_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
