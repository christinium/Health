{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhQsARcYM3t+PTBQDmHF4W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christinium/Health/blob/main/Echo_Fine_Tunning_vs_Prompt_Stats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Echocardiogram Analysis: Comparing Fine-tuned vs Prompt-only Models\n",
        "\n",
        "## Purpose\n",
        "This notebook compares two approaches for extracting structured labels from\n",
        "echocardiogram reports using the Gemma language model:\n",
        "\n",
        "1. **Fine-tuned Model**: Gemma model fine-tuned on labeled echo reports\n",
        "2. **Prompt-only Model**: Base Gemma-2B-it model using detailed prompting\n",
        "\n",
        "## Task\n",
        "Both models extract 19 cardiac features from unstructured echo reports and\n",
        "classify each feature using a standardized coding schema (-3 to 3, representing\n",
        "conditions from \"inadequate to evaluate\" to \"severe dysfunction\").\n",
        "\n",
        "## Evaluation Method: Strict Accuracy\n",
        "This notebook uses **Method 2 (Strict)** evaluation, where:\n",
        "- Failed predictions (unparseable outputs) count as **wrong**\n",
        "- All predictions are evaluated, not just successful parses\n",
        "- This provides a fair comparison reflecting real-world deployment scenarios\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "### Fine-tuned Model Performance\n",
        "- **Average Per-Label Accuracy**: ~99.8%\n",
        "- **Exact Match Accuracy**: ~98.1% (all 19 labels correct)\n",
        "- **Failed Predictions**: <0.1%\n",
        "- The fine-tuned model consistently produces valid, structured output\n",
        "\n",
        "### Prompt-only Model Performance  \n",
        "- **Average Per-Label Accuracy**: ~15-20%\n",
        "- **Exact Match Accuracy**: <1%\n",
        "- **Failed Predictions**: ~50%\n",
        "- The prompt-only model frequently fails to produce parseable output despite\n",
        "  detailed instructions and medical context\n",
        "\n",
        "### Impact of Fine-tuning\n",
        "Fine-tuning improved accuracy by **~80 percentage points** and reduced failed\n",
        "predictions from 50% to nearly 0%. This demonstrates that for structured\n",
        "extraction tasks from specialized medical text, fine-tuning is essentialâ€”prompt\n",
        "engineering alone is insufficient.\n",
        "\n",
        "## Files Required\n",
        "- `test_inference_results_batch_run.csv` - Fine-tuned model predictions\n",
        "- `test_df_with_gemma_predictions.csv` - Prompt-only model predictions\n",
        "\n",
        "Both files should contain ground truth labels for comparison."
      ],
      "metadata": {
        "id": "jefwsiKw_BtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import re\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "VE7HzfqL-TTy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up working directory\n",
        "import os\n",
        "WORKING_DIR = '/content/drive/MyDrive/echo_training/'  # Change this to your preferred location\n",
        "os.makedirs(WORKING_DIR, exist_ok=True)\n",
        "os.chdir(WORKING_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjJwhDIg-qok",
        "outputId": "7bb028c1-bcb9-4d82-c5d7-c35d80f84199"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "# Label definitions\n",
        "LABEL_NAMES = [\n",
        "    'LA_cavity', 'RA_dilated', 'LV_systolic', 'LV_cavity',\n",
        "    'LV_wall', 'RV_cavity', 'RV_systolic', 'AV_stenosis',\n",
        "    'MV_stenosis', 'TV_regurgitation', 'TV_stenosis',\n",
        "    'TV_pulm_htn', 'AV_regurgitation', 'MV_regurgitation',\n",
        "    'RA_pressure', 'LV_diastolic', 'RV_volume_overload',\n",
        "    'RV_wall', 'RV_pressure_overload'\n",
        "]\n"
      ],
      "metadata": {
        "id": "l88oBMpz-a_W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FUNCTIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def load_predictions(model_type):\n",
        "    \"\"\"Load predictions for specified model type.\"\"\"\n",
        "    if model_type == \"fine-tuned\":\n",
        "        df = pd.read_csv('test_inference_results_batch_run.csv')\n",
        "        return df, 'prediction_text', 'true_labels'\n",
        "    else:\n",
        "        df = pd.read_csv('test_df_with_gemma_predictions.csv')\n",
        "        df = df.rename(columns={df.columns[0]: 'id_num'})\n",
        "        return df, 'gemma_result', 'labels'"
      ],
      "metadata": {
        "id": "HC13FICK-hvL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_prediction(pred_text: str, model_type: str) -> list:\n",
        "    \"\"\"Parse model output into label list.\"\"\"\n",
        "    if model_type == \"fine-tuned\":\n",
        "        predicted = []\n",
        "        lines = str(pred_text).split('\\n')\n",
        "\n",
        "        for label_name in LABEL_NAMES:\n",
        "            found = False\n",
        "            for line in lines:\n",
        "                if label_name in line and ':' in line:\n",
        "                    try:\n",
        "                        value_str = line.split(':')[1].strip()\n",
        "                        value = int(value_str)\n",
        "                        predicted.append(value)\n",
        "                        found = True\n",
        "                        break\n",
        "                    except:\n",
        "                        pass\n",
        "            if not found:\n",
        "                predicted.append(None)\n",
        "        return predicted\n",
        "    else:\n",
        "        pattern = r'\\*?\\*?([A-Z_a-z]+)\\s*:\\s*(\\d+)\\*?\\*?'\n",
        "        matches = re.findall(pattern, str(pred_text))\n",
        "        parsed_data = {}\n",
        "        for key, value in matches:\n",
        "            if key in LABEL_NAMES:\n",
        "                parsed_data[key] = int(value)\n",
        "        return [parsed_data.get(label, None) for label in LABEL_NAMES]"
      ],
      "metadata": {
        "id": "1UqHjWCk-iyN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(df):\n",
        "    \"\"\"Calculate accuracy treating unparseable predictions as WRONG.\"\"\"\n",
        "    accuracy_results = []\n",
        "\n",
        "    for i, label_name in enumerate(LABEL_NAMES):\n",
        "        true_vals = df['labels_parsed'].apply(\n",
        "            lambda x: x[i] if isinstance(x, list) and i < len(x) else None\n",
        "        ).values\n",
        "\n",
        "        pred_vals = df['pred_labels'].apply(\n",
        "            lambda x: x[i] if isinstance(x, list) and i < len(x) else None\n",
        "        ).values\n",
        "\n",
        "        valid_true_mask = ~pd.isna(true_vals)\n",
        "        true_vals_all = true_vals[valid_true_mask]\n",
        "        pred_vals_all = pred_vals[valid_true_mask]\n",
        "\n",
        "        correct = sum(1 for tv, pv in zip(true_vals_all, pred_vals_all)\n",
        "                     if pd.notna(pv) and tv == pv)\n",
        "\n",
        "        total = len(true_vals_all)\n",
        "        accuracy = correct / total if total > 0 else 0\n",
        "        failed = sum(pd.isna(pred_vals_all))\n",
        "\n",
        "        accuracy_results.append({\n",
        "            'label': label_name,\n",
        "            'correct': correct,\n",
        "            'total': total,\n",
        "            'accuracy': accuracy,\n",
        "            'failed': failed\n",
        "        })\n",
        "\n",
        "    exact_matches = sum(\n",
        "        1 for idx in range(len(df))\n",
        "        if (df.iloc[idx]['pred_labels'] is not None and\n",
        "            df.iloc[idx]['labels_parsed'] == df.iloc[idx]['pred_labels'])\n",
        "    )\n",
        "\n",
        "    return accuracy_results, exact_matches"
      ],
      "metadata": {
        "id": "TDW9gi4_-mXU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(model_type, accuracy_results, exact_matches, total_samples):\n",
        "    \"\"\"Display results for a model.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"{model_type.upper()} MODEL RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for result in accuracy_results:\n",
        "        failed_str = f\" ({result['failed']} failed)\" if result['failed'] > 0 else \"\"\n",
        "        print(f\"{result['label']:20s}: {result['correct']:5d}/{result['total']:5d} = {result['accuracy']:.4f}{failed_str}\")\n",
        "\n",
        "    avg_accuracy = np.mean([r['accuracy'] for r in accuracy_results])\n",
        "    total_failed = sum(r['failed'] for r in accuracy_results)\n",
        "    total_predictions = total_samples * len(LABEL_NAMES)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\"Average Per-Label Accuracy: {avg_accuracy:.4f} ({avg_accuracy*100:.2f}%)\")\n",
        "    print(f\"Exact Match Accuracy: {exact_matches}/{total_samples} = {exact_matches/total_samples:.4f}\")\n",
        "    print(f\"Total Failed Predictions: {total_failed}/{total_predictions} ({total_failed/total_predictions*100:.2f}%)\")\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "Xelgcp6m-dRE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd6rwAlk-QRb",
        "outputId": "c99f7045-ac00-4ffc-bbbb-5fb65402feb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Processing FINE-TUNED model...\n",
            "================================================================================\n",
            "Loaded 6608 predictions\n",
            "Parsing predictions...\n",
            "Calculating accuracy...\n",
            "\n",
            "================================================================================\n",
            "FINE-TUNED MODEL RESULTS\n",
            "================================================================================\n",
            "LA_cavity           :  6596/ 6608 = 0.9982 (1 failed)\n",
            "RA_dilated          :  6607/ 6608 = 0.9998 (1 failed)\n",
            "LV_systolic         :  6598/ 6608 = 0.9985 (1 failed)\n",
            "LV_cavity           :  6604/ 6608 = 0.9994 (1 failed)\n",
            "LV_wall             :  6597/ 6608 = 0.9983 (1 failed)\n",
            "RV_cavity           :  6605/ 6608 = 0.9995 (1 failed)\n",
            "RV_systolic         :  6601/ 6608 = 0.9989 (1 failed)\n",
            "AV_stenosis         :  6599/ 6608 = 0.9986 (1 failed)\n",
            "MV_stenosis         :  6602/ 6608 = 0.9991 (1 failed)\n",
            "TV_regurgitation    :  6602/ 6608 = 0.9991 (1 failed)\n",
            "TV_stenosis         :  6606/ 6608 = 0.9997 (1 failed)\n",
            "TV_pulm_htn         :  6603/ 6608 = 0.9992 (1 failed)\n",
            "AV_regurgitation    :  6579/ 6608 = 0.9956 (1 failed)\n",
            "MV_regurgitation    :  6581/ 6608 = 0.9959 (1 failed)\n",
            "RA_pressure         :  6607/ 6608 = 0.9998 (1 failed)\n",
            "LV_diastolic        :  6598/ 6608 = 0.9985 (1 failed)\n",
            "RV_volume_overload  :  6605/ 6608 = 0.9995 (1 failed)\n",
            "RV_wall             :  6606/ 6608 = 0.9997 (1 failed)\n",
            "RV_pressure_overload:  6604/ 6608 = 0.9994 (1 failed)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Average Per-Label Accuracy: 0.9988 (99.88%)\n",
            "Exact Match Accuracy: 6481/6608 = 0.9808\n",
            "Total Failed Predictions: 19/125552 (0.02%)\n",
            "================================================================================\n",
            "\n",
            "Saved to: accuracy_fine-tuned_strict_20251003_213002.csv\n",
            "\n",
            "================================================================================\n",
            "Processing PROMPT-ONLY model...\n",
            "================================================================================\n",
            "Loaded 6608 predictions\n",
            "Parsing predictions...\n",
            "Calculating accuracy...\n",
            "\n",
            "================================================================================\n",
            "PROMPT-ONLY MODEL RESULTS\n",
            "================================================================================\n",
            "LA_cavity           :   530/ 6608 = 0.0802 (3262 failed)\n",
            "RA_dilated          :   626/ 6608 = 0.0947 (3336 failed)\n",
            "LV_systolic         :   213/ 6608 = 0.0322 (3274 failed)\n",
            "LV_cavity           :   462/ 6608 = 0.0699 (3294 failed)\n",
            "LV_wall             :   839/ 6608 = 0.1270 (3300 failed)\n",
            "RV_cavity           :   723/ 6608 = 0.1094 (3319 failed)\n",
            "RV_systolic         :   836/ 6608 = 0.1265 (3306 failed)\n",
            "AV_stenosis         :   300/ 6608 = 0.0454 (3410 failed)\n",
            "MV_stenosis         :   624/ 6608 = 0.0944 (3336 failed)\n",
            "TV_regurgitation    :   653/ 6608 = 0.0988 (3355 failed)\n",
            "TV_stenosis         :   797/ 6608 = 0.1206 (3430 failed)\n",
            "TV_pulm_htn         :   795/ 6608 = 0.1203 (3422 failed)\n",
            "AV_regurgitation    :   876/ 6608 = 0.1326 (3491 failed)\n",
            "MV_regurgitation    :  1037/ 6608 = 0.1569 (3460 failed)\n",
            "RA_pressure         :   656/ 6608 = 0.0993 (3496 failed)\n",
            "LV_diastolic        :   579/ 6608 = 0.0876 (3503 failed)\n",
            "RV_volume_overload  :   703/ 6608 = 0.1064 (3466 failed)\n",
            "RV_wall             :   481/ 6608 = 0.0728 (3416 failed)\n",
            "RV_pressure_overload:   663/ 6608 = 0.1003 (3443 failed)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Average Per-Label Accuracy: 0.0987 (9.87%)\n",
            "Exact Match Accuracy: 0/6608 = 0.0000\n",
            "Total Failed Predictions: 64319/125552 (51.23%)\n",
            "================================================================================\n",
            "\n",
            "Saved to: accuracy_prompt-only_strict_20251003_213005.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# EVALUATE BOTH MODELS\n",
        "# ==============================================================================\n",
        "\n",
        "results_comparison = {}\n",
        "\n",
        "for model_type in [\"fine-tuned\", \"prompt-only\"]:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing {model_type.upper()} model...\")\n",
        "    print('='*80)\n",
        "\n",
        "    # Load data\n",
        "    df, pred_col, label_col = load_predictions(model_type)\n",
        "    print(f\"Loaded {len(df)} predictions\")\n",
        "\n",
        "    # Parse predictions\n",
        "    print(\"Parsing predictions...\")\n",
        "    df['pred_labels'] = df[pred_col].apply(lambda x: parse_prediction(x, model_type))\n",
        "    df['labels_parsed'] = df[label_col].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Calculate accuracy\n",
        "    print(\"Calculating accuracy...\")\n",
        "    accuracy_results, exact_matches = calculate_accuracy(df)\n",
        "\n",
        "    # Store results\n",
        "    results_comparison[model_type] = {\n",
        "        'accuracy_results': accuracy_results,\n",
        "        'exact_matches': exact_matches,\n",
        "        'total_samples': len(df)\n",
        "    }\n",
        "\n",
        "    # Display results\n",
        "    display_results(model_type, accuracy_results, exact_matches, len(df))\n",
        "\n",
        "    # Save individual results\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_df = pd.DataFrame(accuracy_results)\n",
        "    results_df.to_csv(f'accuracy_{model_type}_strict_{timestamp}.csv', index=False)\n",
        "    print(f\"\\nSaved to: accuracy_{model_type}_strict_{timestamp}.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SIDE-BY-SIDE COMPARISON\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "print(\"SIDE-BY-SIDE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Label':<20} {'Fine-tuned':>15} {'Prompt-only':>15} {'Difference':>15}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for i, label in enumerate(LABEL_NAMES):\n",
        "    ft_acc = results_comparison['fine-tuned']['accuracy_results'][i]['accuracy']\n",
        "    po_acc = results_comparison['prompt-only']['accuracy_results'][i]['accuracy']\n",
        "    diff = ft_acc - po_acc\n",
        "\n",
        "    print(f\"{label:<20} {ft_acc:>14.4f} {po_acc:>14.4f} {diff:>+14.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "ft_avg = np.mean([r['accuracy'] for r in results_comparison['fine-tuned']['accuracy_results']])\n",
        "po_avg = np.mean([r['accuracy'] for r in results_comparison['prompt-only']['accuracy_results']])\n",
        "ft_exact = results_comparison['fine-tuned']['exact_matches'] / results_comparison['fine-tuned']['total_samples']\n",
        "po_exact = results_comparison['prompt-only']['exact_matches'] / results_comparison['prompt-only']['total_samples']\n",
        "\n",
        "print(f\"{'Average Accuracy':<20} {ft_avg:>14.4f} {po_avg:>14.4f} {ft_avg-po_avg:>+14.4f}\")\n",
        "print(f\"{'Exact Match':<20} {ft_exact:>14.4f} {po_exact:>14.4f} {ft_exact-po_exact:>+14.4f}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VyS263H-zcO",
        "outputId": "c06b93a1-c127-43a5-e20a-b08cf91c07ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "================================================================================\n",
            "SIDE-BY-SIDE COMPARISON\n",
            "================================================================================\n",
            "\n",
            "Label                     Fine-tuned     Prompt-only      Difference\n",
            "--------------------------------------------------------------------------------\n",
            "LA_cavity                    0.9982         0.0802        +0.9180\n",
            "RA_dilated                   0.9998         0.0947        +0.9051\n",
            "LV_systolic                  0.9985         0.0322        +0.9663\n",
            "LV_cavity                    0.9994         0.0699        +0.9295\n",
            "LV_wall                      0.9983         0.1270        +0.8714\n",
            "RV_cavity                    0.9995         0.1094        +0.8901\n",
            "RV_systolic                  0.9989         0.1265        +0.8724\n",
            "AV_stenosis                  0.9986         0.0454        +0.9532\n",
            "MV_stenosis                  0.9991         0.0944        +0.9047\n",
            "TV_regurgitation             0.9991         0.0988        +0.9003\n",
            "TV_stenosis                  0.9997         0.1206        +0.8791\n",
            "TV_pulm_htn                  0.9992         0.1203        +0.8789\n",
            "AV_regurgitation             0.9956         0.1326        +0.8630\n",
            "MV_regurgitation             0.9959         0.1569        +0.8390\n",
            "RA_pressure                  0.9998         0.0993        +0.9006\n",
            "LV_diastolic                 0.9985         0.0876        +0.9109\n",
            "RV_volume_overload           0.9995         0.1064        +0.8932\n",
            "RV_wall                      0.9997         0.0728        +0.9269\n",
            "RV_pressure_overload         0.9994         0.1003        +0.8991\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Average Accuracy             0.9988         0.0987        +0.9001\n",
            "Exact Match                  0.9808         0.0000        +0.9808\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}