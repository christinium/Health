{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9e6bbe",
   "metadata": {},
   "source": [
    "# Echo Note Training with Gemma LLM\n",
    "This notebook demonstrates how to fine-tune the Gemma language model on echocardiogram notes. The model will learn to analyze echo reports and provide structured assessments across 19 different cardiac features.\n",
    "\n",
    "## Overview\n",
    "1. Set up Google Drive and dependencies\n",
    "2. Load and prepare echo note data\n",
    "3. Configure data labels and formatting\n",
    "4. Clean and preprocess the dataset\n",
    "5. Create train/tune/test splits\n",
    "6. Convert to HuggingFace format\n",
    "7. Configure and train the model\n",
    "8. Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b6de9",
   "metadata": {},
   "source": [
    "## 1. Set Up Google Drive\n",
    "First, let's mount Google Drive to access our data and save model checkpoints. This step is essential for persisting our data and model files across Colab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae06bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up working directory\n",
    "import os\n",
    "WORKING_DIR = '/content/drive/MyDrive/echo_training'  # Change this to your preferred location\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "os.chdir(WORKING_DIR)\n",
    "\n",
    "print(f\"Working directory set to: {WORKING_DIR}\")\n",
    "print(\"Contents:\", os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfda89f",
   "metadata": {},
   "source": [
    "## 2. Install Required Dependencies\n",
    "Install and import all necessary libraries for the project. We'll need:\n",
    "- transformers: For the Gemma model and training\n",
    "- datasets: For data handling\n",
    "- pandas & numpy: For data manipulation\n",
    "- sklearn: For data splitting\n",
    "- torch: For deep learning operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a408035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch numpy pandas scikit-learn bitsandbytes -q\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling, TrainingArguments\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67edf4a",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Echo Data\n",
    "Load your echo data from the CSV file. Make sure your data file is uploaded to the working directory in Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV file\n",
    "# Change 'your_echo_data.csv' to your actual filename in Google Drive\n",
    "df = pd.read_csv('your_echo_data.csv')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(2).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e8944",
   "metadata": {},
   "source": [
    "## 4. Configure Data Labels\n",
    "Set up the 19 cardiac feature labels and create functions to parse the label data. These labels represent different aspects of cardiac function that we'll be predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label names\n",
    "LABEL_NAMES = [\n",
    "    'LA_cavity', 'RA_dilated', 'LV_systolic', 'LV_cavity',\n",
    "    'LV_wall', 'RV_cavity', 'RV_systolic', 'AV_stenosis',\n",
    "    'MV_stenosis', 'TV_regurgitation', 'TV_stenosis',\n",
    "    'TV_pulm_htn', 'AV_regurgitation', 'MV_regurgitation',\n",
    "    'RA_pressure', 'LV_diastolic', 'RV_volume_overload',\n",
    "    'RV_wall', 'RV_pressure_overload'\n",
    "]\n",
    "\n",
    "def parse_labels(label_str):\n",
    "    \"\"\"Convert string representation of list to actual list.\"\"\"\n",
    "    if isinstance(label_str, str):\n",
    "        return ast.literal_eval(label_str)\n",
    "    elif isinstance(label_str, list):\n",
    "        return label_str\n",
    "    else:\n",
    "        return label_str\n",
    "\n",
    "# Parse labels\n",
    "df['labels_parsed'] = df['labels'].apply(parse_labels)\n",
    "\n",
    "# Verify labels\n",
    "assert all(len(labels) == 19 for labels in df['labels_parsed']), \\\n",
    "    \"Not all label arrays have 19 values!\"\n",
    "\n",
    "print(\"\\nLabels parsed successfully!\")\n",
    "print(f\"Example labels: {df['labels_parsed'].iloc[0]}\")\n",
    "\n",
    "# Display label distribution\n",
    "label_counts = pd.DataFrame([\n",
    "    [label, sum(x[i] for x in df['labels_parsed'])] \n",
    "    for i, label in enumerate(LABEL_NAMES)\n",
    "], columns=['Label', 'Count'])\n",
    "\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef08f1",
   "metadata": {},
   "source": [
    "## 5. Format Data for Training\n",
    "Create a formatting function to prepare the data for Gemma. We'll structure the input as a prompt with the echo report and expected output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_echo_prompt(row):\n",
    "    \"\"\"Format echo report into Gemma instruction format.\"\"\"\n",
    "    input_text = row['input']  # Change 'input' to your actual column name\n",
    "    labels = row['labels_parsed']\n",
    "    \n",
    "    # Create formatted label string\n",
    "    label_pairs = [f\"{LABEL_NAMES[i]}: {labels[i]}\" for i in range(19)]\n",
    "    label_text = \"\\n\".join(label_pairs)\n",
    "    \n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "Analyze this echocardiogram report and provide assessment values for each cardiac feature. Output should be in the format \"feature: value\" for each of the 19 features.\n",
    "\n",
    "Report:\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{label_text}<end_of_turn>\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Apply formatting\n",
    "df['text'] = df.apply(format_echo_prompt, axis=1)\n",
    "\n",
    "print(\"FORMATTED EXAMPLE:\")\n",
    "print(\"=\"*70)\n",
    "print(df['text'].iloc[0])\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f406ff",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning and Preprocessing\n",
    "Clean the dataset by removing duplicates, handling missing values, and filtering by text length if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before cleaning: {len(df)} samples\")\n",
    "\n",
    "# Remove any rows with missing data\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Remove duplicates based on input text\n",
    "df = df.drop_duplicates(subset=['input'])\n",
    "\n",
    "# Analyze text length\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(f\"\\nText length stats:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "# Optional: Remove extremely short or long examples\n",
    "# Uncomment and adjust thresholds as needed\n",
    "# df = df[(df['text_length'] > 100) & (df['text_length'] < 4096)]\n",
    "\n",
    "df = df.drop(columns=['text_length'])\n",
    "\n",
    "print(f\"After cleaning: {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a0853",
   "metadata": {},
   "source": [
    "## 7. Create Dataset Splits\n",
    "Split the data into training (70%), tuning/validation (15%), and test (15%) sets. We'll save these splits to CSV files in Google Drive for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: separate test set (15%)\n",
    "train_tune_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: separate train and tune from remaining 85%\n",
    "train_df, tune_df = train_test_split(\n",
    "    train_tune_df,\n",
    "    test_size=0.1765,  # This gives us 15% of original\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"DATASET SPLITS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set:   {len(train_df)} samples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Tuning set:     {len(tune_df)} samples ({len(tune_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set:       {len(test_df)} samples ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Total:          {len(df)} samples\")\n",
    "\n",
    "# Save splits to CSV\n",
    "train_df.to_csv('echo_train.csv', index=False)\n",
    "tune_df.to_csv('echo_tune.csv', index=False)\n",
    "test_df.to_csv('echo_test.csv', index=False)\n",
    "\n",
    "print(\"\\n✓ Splits saved to CSV files in Google Drive:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bca897",
   "metadata": {},
   "source": [
    "## 8. Convert to HuggingFace Format\n",
    "Transform our data into HuggingFace datasets for efficient training. This includes creating a DatasetDict and tokenizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets from the DataFrames\n",
    "train_dataset = Dataset.from_pandas(train_df[['text']], preserve_index=False)\n",
    "tune_dataset = Dataset.from_pandas(tune_df[['text']], preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_df[['text']], preserve_index=False)\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': tune_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(\"HUGGING FACE DATASET:\")\n",
    "print(\"=\"*70)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5215730",
   "metadata": {},
   "source": [
    "## 9. Configure Model and Tokenizer\n",
    "Set up the Gemma model and tokenizer. We'll use the 2B parameter version for prototyping, but you can switch to 9B for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = 'google/gemma-2b'  # or 'google/gemma-9b' for production\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,  # Changed to bfloat16\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing after model creation\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False  # Disable KV cache for training\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text data.\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,  # Reduced from 1024 to save memory\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "# Tokenize all splits\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing datasets\",\n",
    ")\n",
    "\n",
    "print(\"TOKENIZED DATASETS:\")\n",
    "print(\"=\"*70)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231bb538",
   "metadata": {},
   "source": [
    "## 10. Configure Training\n",
    "Set up the training arguments and create the trainer. We'll use settings optimized for medical domain training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a133dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Training arguments optimized for medical domain with memory constraints\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma_echo_finetuned\",\n",
    "    num_train_epochs=5,\n",
    "    \n",
    "    # Reduced batch sizes\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Memory optimization settings\n",
    "    fp16=False,          # Disable fp16\n",
    "    bf16=True,          # Enable bf16 instead\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    \n",
    "    # Evaluation and saving settings\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    # Training optimization\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    report_to=\"none\",\n",
    "    hub_strategy=\"end\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Trainer configured for echo report data!\")\n",
    "print(f\"Training on:   {len(tokenized_datasets['train'])} samples\")\n",
    "print(f\"Validating on: {len(tokenized_datasets['validation'])} samples\")\n",
    "print(f\"Test set:      {len(tokenized_datasets['test'])} samples (held out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6724bd",
   "metadata": {},
   "source": [
    "## 11. Model Training and Evaluation\n",
    "Train the model and evaluate its performance. We'll also include a helper function for final evaluation on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcb9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "def evaluate_on_test_set():\n",
    "    \"\"\"Evaluate the fine-tuned model on the held-out test set.\"\"\"\n",
    "    print(\"\\nEVALUATING ON TEST SET\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_results = trainer.evaluate(tokenized_datasets['test'])\n",
    "    \n",
    "    print(\"\\nTest Set Results:\")\n",
    "    for key, value in test_results.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = evaluate_on_test_set()\n",
    "\n",
    "# Save the final model\n",
    "output_dir = \"final_model\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\n✓ Model and tokenizer saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d3eac",
   "metadata": {},
   "source": [
    "## Important Notes for Medical Data\n",
    "\n",
    "1. **Data Balance**\n",
    "   - Check if your 19 labels are balanced\n",
    "   - Medical data often has class imbalance\n",
    "   - Consider stratified splitting if certain conditions are rare\n",
    "\n",
    "2. **Evaluation Metrics**\n",
    "   - Loss alone may not be sufficient\n",
    "   - Consider implementing custom metrics (F1, precision, recall per label)\n",
    "   - Medical predictions need high precision\n",
    "\n",
    "3. **Model Size**\n",
    "   - Gemma 2B is good for prototyping\n",
    "   - Consider Gemma 9B for production if accuracy is critical\n",
    "   \n",
    "4. **Training Tips**\n",
    "   - Monitor validation loss closely\n",
    "   - Stop if validation loss stops decreasing\n",
    "   - Medical domain may need more epochs (5-10)\n",
    "   - Lower learning rate is safer for specialized domains\n",
    "\n",
    "5. **Prompt Engineering**\n",
    "   - Current format outputs all 19 values at once\n",
    "   - Consider one value at a time for higher reliability\n",
    "   - Add medical context in prompts if needed\n",
    "\n",
    "Remember to validate the model thoroughly before any clinical use!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
