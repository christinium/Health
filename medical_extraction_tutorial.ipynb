{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a1519f",
   "metadata": {},
   "source": [
    "# Medical Text Extraction with Small Language Models\n",
    "\n",
    "This tutorial demonstrates how to fine-tune a small language model (3-7B parameters) for extracting structured information from medical texts, specifically focusing on echocardiography reports from MIMIC-III.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Setup & Data Connection**\n",
    "   - Environment setup with Unsloth for T4 GPU optimization\n",
    "   - MIMIC-III data access and echo report extraction\n",
    "   - BigQuery connection setup\n",
    "\n",
    "2. **Data Preprocessing**\n",
    "   - Text cleaning and standardization\n",
    "   - Structured data formatting\n",
    "   - Training data preparation\n",
    "\n",
    "3. **Model Training**\n",
    "   - Small LLM loading and configuration\n",
    "   - LoRA setup for efficient fine-tuning\n",
    "   - Training pipeline implementation\n",
    "\n",
    "4. **Evaluation & Testing**\n",
    "   - Model evaluation on test set\n",
    "   - Metric computation and analysis\n",
    "   - Error analysis and quality assessment\n",
    "\n",
    "> **Note**: This notebook requires a GPU runtime. Please select Runtime > Change runtime type > GPU before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f998e",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Connection\n",
    "\n",
    "First, let's set up our environment with the necessary packages. We'll use Unsloth for optimized training on T4 GPUs and install other required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06820ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch==2.1.2 accelerate==0.27.0 bitsandbytes==0.41.3\n",
    "!pip install -q unsloth\n",
    "!pip install -q google-cloud-bigquery pandas numpy tqdm scikit-learn\n",
    "!pip install -q transformers==4.38.2 datasets==2.16.1\n",
    "!pip install -q wandb  # For experiment tracking\n",
    "\n",
    "import IPython\n",
    "IPython.display.clear_output()\n",
    "print(\"âœ… Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3b7d0",
   "metadata": {},
   "source": [
    "### Connect to BigQuery and Extract MIMIC-III Data\n",
    "\n",
    "Now we'll set up the connection to BigQuery and extract echocardiography reports from MIMIC-III. You'll need to have access to the MIMIC-III dataset in BigQuery and appropriate credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# SQL query to extract echo reports\n",
    "QUERY = \"\"\"\n",
    "SELECT \n",
    "    n.row_id,\n",
    "    n.subject_id,\n",
    "    n.hadm_id,\n",
    "    n.text\n",
    "FROM `physionet-data.mimiciii_notes.noteevents` n\n",
    "WHERE n.category = 'Echo'\n",
    "    AND n.text IS NOT NULL\n",
    "    AND CHAR_LENGTH(n.text) > 100\n",
    "LIMIT 1000  # Adjust based on your needs\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and load into DataFrame\n",
    "df_echo = client.query(QUERY).to_dataframe()\n",
    "print(f\"Retrieved {len(df_echo)} echo reports\")\n",
    "\n",
    "# Save a sample report\n",
    "sample_report = df_echo.iloc[0]['text']\n",
    "print(\"\\nSample Echo Report (truncated):\")\n",
    "print(\"=\"*80)\n",
    "print(sample_report[:500], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6c92c6",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Now we'll clean and prepare the echo reports for training. We'll:\n",
    "1. Clean and standardize the text\n",
    "2. Extract key measurements and findings\n",
    "3. Create a structured JSON schema\n",
    "4. Format the data for instruction fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_echo_text(text):\n",
    "    \"\"\"Clean and standardize echo report text.\"\"\"\n",
    "    # Remove redundant whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Standardize common measurements\n",
    "    text = text.replace('ejection fraction', 'EF')\n",
    "    text = text.replace('left ventricular', 'LV')\n",
    "    text = text.replace('right ventricular', 'RV')\n",
    "    \n",
    "    # Convert to lowercase for consistency\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_measurements(text):\n",
    "    \"\"\"Extract key measurements from echo report.\"\"\"\n",
    "    measurements = {\n",
    "        'ef': None,\n",
    "        'lv_size': None,\n",
    "        'rv_size': None,\n",
    "        'valve_status': {},\n",
    "    }\n",
    "    \n",
    "    # Example simple pattern matching (in practice, use more robust NLP)\n",
    "    if 'ef' in text:\n",
    "        # Look for EF percentage\n",
    "        import re\n",
    "        ef_match = re.search(r'ef[:\\s]+(\\d{1,2})[-%]', text)\n",
    "        if ef_match:\n",
    "            measurements['ef'] = int(ef_match.group(1))\n",
    "    \n",
    "    return measurements\n",
    "\n",
    "# Process the echo reports\n",
    "processed_data = []\n",
    "for _, row in tqdm(df_echo.iterrows(), total=len(df_echo)):\n",
    "    clean_text = clean_echo_text(row['text'])\n",
    "    measurements = extract_measurements(clean_text)\n",
    "    \n",
    "    processed_data.append({\n",
    "        'report_id': row['row_id'],\n",
    "        'text': clean_text,\n",
    "        'measurements': measurements\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_processed = pd.DataFrame(processed_data)\n",
    "print(f\"\\nProcessed {len(df_processed)} reports\")\n",
    "print(\"\\nSample processed data:\")\n",
    "print(json.dumps(df_processed.iloc[0].to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea7b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for instruction fine-tuning\n",
    "def create_instruction_format(row):\n",
    "    \"\"\"Create instruction-input-output format for training.\"\"\"\n",
    "    instruction = \"\"\"Extract key measurements and findings from the following echocardiogram report. \n",
    "                    Return the results in JSON format including: EF, LV size, RV size, and valve status.\"\"\"\n",
    "    \n",
    "    input_text = row['text']\n",
    "    \n",
    "    # Format output as a clean JSON string\n",
    "    output = json.dumps(row['measurements'], indent=2)\n",
    "    \n",
    "    return {\n",
    "        'instruction': instruction,\n",
    "        'input': input_text,\n",
    "        'output': output\n",
    "    }\n",
    "\n",
    "# Create training data\n",
    "training_data = [create_instruction_format(row) for _, row in df_processed.iterrows()]\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(training_data, test_size=0.3, random_state=SEED)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=SEED)\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"Training: {len(train_data)}\")\n",
    "print(f\"Validation: {len(val_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# Save a sample for reference\n",
    "print(\"\\nSample training instance:\")\n",
    "print(json.dumps(train_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701d71c9",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Now we'll set up and train our model using Unsloth's optimized training pipeline. We'll use a small language model (Phi-2) and configure it for efficient fine-tuning with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"microsoft/phi-2\"  # 2.7B parameter model\n",
    "OUTPUT_DIR = \"medical_extraction_model\"\n",
    "\n",
    "# Configure quantization for efficient training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model and tokenizer with Unsloth optimization\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_sequence_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    warmup_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\"  # Enable W&B logging\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"Model and training configuration complete!\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"Training on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model():\n",
    "    # Initialize Weights & Biases\n",
    "    import wandb\n",
    "    wandb.init(project=\"medical-extraction\", name=\"phi2-echo-extraction\")\n",
    "    \n",
    "    # Create the trainer\n",
    "    trainer = FastLanguageModel.get_trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Start training\n",
    "trainer = train_model()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c866e84",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Testing\n",
    "\n",
    "Now we'll evaluate our model's performance on the test set and analyze its accuracy in extracting medical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extraction(true_json, pred_json):\n",
    "    \"\"\"Evaluate the accuracy of extracted fields.\"\"\"\n",
    "    metrics = {\n",
    "        'ef_accuracy': 0,\n",
    "        'size_accuracy': 0,\n",
    "        'valve_accuracy': 0,\n",
    "        'overall_accuracy': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        true_data = json.loads(true_json) if isinstance(true_json, str) else true_json\n",
    "        pred_data = json.loads(pred_json) if isinstance(pred_json, str) else pred_json\n",
    "        \n",
    "        # Check EF accuracy\n",
    "        if true_data['ef'] == pred_data['ef']:\n",
    "            metrics['ef_accuracy'] = 1\n",
    "            \n",
    "        # Check size measurements\n",
    "        size_correct = (true_data['lv_size'] == pred_data['lv_size'] and\n",
    "                       true_data['rv_size'] == pred_data['rv_size'])\n",
    "        metrics['size_accuracy'] = 1 if size_correct else 0\n",
    "        \n",
    "        # Check valve status\n",
    "        valve_correct = true_data['valve_status'] == pred_data['valve_status']\n",
    "        metrics['valve_accuracy'] = 1 if valve_correct else 0\n",
    "        \n",
    "        # Calculate overall accuracy\n",
    "        metrics['overall_accuracy'] = sum([\n",
    "            metrics['ef_accuracy'],\n",
    "            metrics['size_accuracy'],\n",
    "            metrics['valve_accuracy']\n",
    "        ]) / 3\n",
    "        \n",
    "    except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "        print(f\"Error in evaluation: {e}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Generate predictions on test set\n",
    "def generate_predictions(model, test_samples):\n",
    "    predictions = []\n",
    "    \n",
    "    for sample in tqdm(test_samples, desc=\"Generating predictions\"):\n",
    "        input_text = f\"{sample['instruction']}\\n\\n{sample['input']}\"\n",
    "        \n",
    "        # Generate prediction\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Evaluate model on test set\n",
    "predictions = generate_predictions(model, test_data)\n",
    "\n",
    "# Calculate metrics\n",
    "all_metrics = []\n",
    "for test_sample, pred in zip(test_data, predictions):\n",
    "    metrics = evaluate_extraction(test_sample['output'], pred)\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = {\n",
    "    key: np.mean([m[key] for m in all_metrics])\n",
    "    for key in all_metrics[0].keys()\n",
    "}\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"{metric}: {value:.2%}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(min(3, len(test_data))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Input:\", test_data[i]['input'][:200], \"...\")\n",
    "    print(\"\\nTrue Output:\", test_data[i]['output'])\n",
    "    print(\"\\nPredicted Output:\", predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232686c3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated how to:\n",
    "1. Set up a medical text extraction pipeline using a small language model\n",
    "2. Process and prepare MIMIC-III echocardiography data\n",
    "3. Fine-tune the model efficiently using LoRA and Unsloth\n",
    "4. Evaluate the model's performance on structured information extraction\n",
    "\n",
    "The trained model can be used to automatically extract key measurements and findings from echocardiography reports, helping to standardize and structure medical information.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Experiment with different model architectures (e.g., Llama-2-7b, MPT-7b)\n",
    "2. Improve the extraction patterns for better accuracy\n",
    "3. Add more structured fields to extract\n",
    "4. Implement clinical validation metrics\n",
    "5. Deploy the model in a clinical setting with proper validation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
