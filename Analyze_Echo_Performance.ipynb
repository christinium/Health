{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMj3yquWVDsHHtp+3FdmbAH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christinium/Health/blob/main/Analyze_Echo_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Echo Results Review Try 2"
      ],
      "metadata": {
        "id": "r7zpJ0SAyRXS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWe6XAZ-xpBN",
        "outputId": "81f948a9-e566-48d7-e9d0-12a45d4443d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up working directory\n",
        "import os\n",
        "WORKING_DIR = '/content/drive/MyDrive/echo_training/'  # Change this to your preferred location\n",
        "os.makedirs(WORKING_DIR, exist_ok=True)\n",
        "os.chdir(WORKING_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "yvGlzCufyfLA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('echo_train.csv')\n",
        "tune_df = pd.read_csv('echo_tune.csv')\n",
        "test_df = pd.read_csv('echo_test.csv')"
      ],
      "metadata": {
        "id": "HEU6Eg7JzBo2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = pd.read_csv('test_results.csv')"
      ],
      "metadata": {
        "id": "z25WaD-2scgn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "AB8zV0KssoAw",
        "outputId": "081ea004-c9d7-4354-8b9c-d8923db63b9b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   eval_loss  eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
              "0   0.116445      137.3173                   48.122                  6.015   \n",
              "\n",
              "   epoch  \n",
              "0    3.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91aa0ea1-c985-440e-a465-34d451231aef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eval_loss</th>\n",
              "      <th>eval_runtime</th>\n",
              "      <th>eval_samples_per_second</th>\n",
              "      <th>eval_steps_per_second</th>\n",
              "      <th>epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.116445</td>\n",
              "      <td>137.3173</td>\n",
              "      <td>48.122</td>\n",
              "      <td>6.015</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91aa0ea1-c985-440e-a465-34d451231aef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-91aa0ea1-c985-440e-a465-34d451231aef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-91aa0ea1-c985-440e-a465-34d451231aef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_098808b7-bda7-4a1e-bdfe-cbc9673bdd8d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_098808b7-bda7-4a1e-bdfe-cbc9673bdd8d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_results",
              "summary": "{\n  \"name\": \"test_results\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"eval_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.1164446398615837,\n        \"max\": 0.1164446398615837,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.1164446398615837\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_runtime\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 137.3173,\n        \"max\": 137.3173,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          137.3173\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_samples_per_second\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 48.122,\n        \"max\": 48.122,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          48.122\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_steps_per_second\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 6.015,\n        \"max\": 6.015,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6.015\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"epoch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_copy = test_df.copy()"
      ],
      "metadata": {
        "id": "Pyp9X_O5zDiu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df_copy = test_df_copy.rename(columns={test_df_copy.columns[0]: 'id_num'})"
      ],
      "metadata": {
        "id": "F5lpo5BGzgCE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# SETUP: LOAD MODEL AND DEFINE LABELS\n",
        "# ==============================================================================\n",
        "\n",
        "# Define label names\n",
        "LABEL_NAMES = [\n",
        "    'LA_cavity', 'RA_dilated', 'LV_systolic', 'LV_cavity',\n",
        "    'LV_wall', 'RV_cavity', 'RV_systolic', 'AV_stenosis',\n",
        "    'MV_stenosis', 'TV_regurgitation', 'TV_stenosis',\n",
        "    'TV_pulm_htn', 'AV_regurgitation', 'MV_regurgitation',\n",
        "    'RA_pressure', 'LV_diastolic', 'RV_volume_overload',\n",
        "    'RV_wall', 'RV_pressure_overload'\n",
        "]\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_path = \"final_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9874bce056cf40aab5c8481aecd9d759",
            "9de45faeb5ab477e9c7bb10c960f93e9",
            "140be6325c6047bcabe102b7c100a1e7",
            "f64f7b373da14c11894bf674221d686c",
            "ce1643d2752c4ddfafa8e02278506bc1",
            "7c42a754b2644780bd9a76b9de4d9cd3",
            "260d4852002946f3b658973dda1db849",
            "1cb8a43a9b2c447fb0158481b74f1578",
            "a463cf1ec3614924857b22f2868cae8e",
            "fd28a03ec0034c0eba3c14a9c338a9b0",
            "2df5da2c0e284b06a7ed5c4d6200f838"
          ]
        },
        "id": "iwDVLfmcyiif",
        "outputId": "f912c1be-cf87-42a7-9457-c523dfa84b73"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9874bce056cf40aab5c8481aecd9d759"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# INFERENCE FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_prediction(text):\n",
        "    prompt = f\"\"\"<start_of_turn>user\n",
        "Analyze this echocardiogram report and provide assessment values for each cardiac feature. Output should be in the format \"feature: value\" for each of the 19 features.\n",
        "\n",
        "Report:\n",
        "{text}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=300,\n",
        "        temperature=0.1,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the model's response\n",
        "    model_output = full_output.split(\"<start_of_turn>model\\n\")[-1].strip()\n",
        "    return model_output\n"
      ],
      "metadata": {
        "id": "zAp7BJopykxw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PARSE PREDICTIONS - SIMPLER VERSION\n",
        "# ==============================================================================\n",
        "\n",
        "def parse_prediction(pred_text):\n",
        "    \"\"\"Extract predicted label values from model output text\"\"\"\n",
        "    predicted = []\n",
        "    lines = pred_text.split('\\n')\n",
        "\n",
        "    for label_name in LABEL_NAMES:\n",
        "        found = False\n",
        "        for line in lines:\n",
        "            if label_name in line and ':' in line:\n",
        "                try:\n",
        "                    # Get text after colon, remove spaces, convert to int\n",
        "                    value_str = line.split(':')[1].strip()\n",
        "                    value = int(value_str)\n",
        "                    predicted.append(value)\n",
        "                    found = True\n",
        "                    break\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if not found:\n",
        "            predicted.append(None)\n",
        "\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "mnxe2a5nynCH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# RUN INFERENCE ON ALL TEST EXAMPLES\n",
        "# ==============================================================================\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"Running inference on {len(test_df_copy)} test examples...\")\n",
        "for idx in tqdm(range(len(test_df_copy))):\n",
        "    # Get data\n",
        "    echo_text = test_df_copy.iloc[idx]['text']\n",
        "    true_labels_raw = test_df_copy.iloc[idx]['labels']\n",
        "    id_num = test_df_copy.iloc[idx]['id_num']\n",
        "\n",
        "    # Parse true labels\n",
        "    if isinstance(true_labels_raw, str):\n",
        "        true_labels = ast.literal_eval(true_labels_raw)\n",
        "    else:\n",
        "        true_labels = true_labels_raw\n",
        "\n",
        "    # Generate prediction\n",
        "    pred_text = generate_prediction(echo_text)\n",
        "\n",
        "    # Store results\n",
        "    result = {\n",
        "        'idx': idx,\n",
        "        'id_num': id_num,\n",
        "        'echo_text': echo_text,\n",
        "        'true_labels': true_labels,\n",
        "        'prediction_text': pred_text\n",
        "    }\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('test_inference_results.csv', index=False)\n",
        "print(f\"\\nSaved results to test_inference_results.csv\")\n",
        "print(f\"Shape: {results_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "iw57Jqw-yqFo",
        "outputId": "d94adc5a-425f-4434-fbfe-b3c72f64d9e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on 6608 test examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6608 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "  1%|          | 75/6608 [09:09<13:17:32,  7.32s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2516721781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Generate prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpred_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecho_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-753286581.py\u001b[0m in \u001b[0;36mgenerate_prediction\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGREEDY_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0;31m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2540\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2868\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2869\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2870\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2872\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;34m\"What is your favorite condiment?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 461\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    398\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     ) -> torch.Tensor:\n\u001b[1;32m    284\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         hidden_states, _ = self.self_attn(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# See https://github.com/huggingface/transformers/pull/29402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36m_norm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# BATCH INFERENCE FUNCTION - OPTIMIZED VERSION\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_predictions_batch(texts, batch_size=8):\n",
        "    \"\"\"\n",
        "    Generate predictions for a batch of texts at once.\n",
        "\n",
        "    Args:\n",
        "        texts: List of echo text strings to process\n",
        "        batch_size: Number of texts to process together (adjust based on GPU memory)\n",
        "\n",
        "    Returns:\n",
        "        List of prediction strings\n",
        "    \"\"\"\n",
        "    # Create prompts for all texts in batch\n",
        "    prompts = []\n",
        "    for text in texts:\n",
        "        prompt = f\"\"\"<start_of_turn>user\n",
        "Analyze this echocardiogram report and provide assessment values for each cardiac feature. Output should be in the format \"feature: value\" for each of the 19 features.\n",
        "\n",
        "Report:\n",
        "{text}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Tokenize all prompts at once with padding\n",
        "    inputs = tokenizer(\n",
        "        prompts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=2048  # Adjust if your reports are longer\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate for entire batch\n",
        "    with torch.no_grad():  # Saves memory\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode all outputs\n",
        "    predictions = []\n",
        "    for output in outputs:\n",
        "        full_output = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        # Extract just the model's response\n",
        "        model_output = full_output.split(\"<start_of_turn>model\\n\")[-1].strip()\n",
        "        predictions.append(model_output)\n",
        "\n",
        "    return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "oIDvbHDzHLGi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# RUN BATCH INFERENCE ON ALL TEST EXAMPLES\n",
        "# ==============================================================================\n",
        "\n",
        "results = []\n",
        "batch_size = 8  # Start with 8, increase to 16 or 32 if you have GPU memory available\n",
        "\n",
        "print(f\"Running BATCH inference on {len(test_df_copy)} test examples...\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Estimated time: ~{(len(test_df_copy) / batch_size) * 7.27 / 3600:.1f} hours (vs 13+ hours single)\")\n",
        "\n",
        "# Process in batches\n",
        "for start_idx in tqdm(range(0, len(test_df_copy), batch_size)):\n",
        "    end_idx = min(start_idx + batch_size, len(test_df_copy))\n",
        "    batch_df = test_df_copy.iloc[start_idx:end_idx]\n",
        "\n",
        "    # Get batch data\n",
        "    batch_texts = batch_df['text'].tolist()\n",
        "    batch_ids = batch_df['id_num'].tolist()\n",
        "    batch_true_labels = batch_df['labels'].tolist()\n",
        "\n",
        "    # Generate predictions for entire batch at once\n",
        "    batch_predictions = generate_predictions_batch(batch_texts, batch_size)\n",
        "\n",
        "    # Store results for each item in batch\n",
        "    for i, (idx, text, true_labels_raw, id_num, pred_text) in enumerate(\n",
        "        zip(range(start_idx, end_idx), batch_texts, batch_true_labels, batch_ids, batch_predictions)\n",
        "    ):\n",
        "        # Parse true labels\n",
        "        if isinstance(true_labels_raw, str):\n",
        "            true_labels = ast.literal_eval(true_labels_raw)\n",
        "        else:\n",
        "            true_labels = true_labels_raw\n",
        "\n",
        "        # Store result\n",
        "        result = {\n",
        "            'idx': idx,\n",
        "            'id_num': id_num,\n",
        "            'echo_text': text,\n",
        "            'true_labels': true_labels,\n",
        "            'prediction_text': pred_text\n",
        "        }\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('test_inference_results.csv', index=False)\n",
        "print(f\"\\nSaved results to test_inference_results.csv\")\n",
        "print(f\"Shape: {results_df.shape}\")\n",
        "print(f\"\\n Batch processing complete! This was much faster than single-item processing!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le2Osf6FHMNc",
        "outputId": "1ee22349-4284-4b99-d9c4-dab3122a8964"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running BATCH inference on 6608 test examples...\n",
            "Batch size: 8\n",
            "Estimated time: ~1.7 hours (vs 13+ hours single)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 826/826 [2:15:08<00:00,  9.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved results to test_inference_results.csv\n",
            "Shape: (6608, 5)\n",
            "\n",
            " Batch processing complete! This was much faster than single-item processing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# PART 1: LABEL DISTRIBUTION IN TEST SET\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LABEL DISTRIBUTION IN TEST SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, label_name in enumerate(LABEL_NAMES):\n",
        "    print(f\"\\n{label_name}:\")\n",
        "\n",
        "    # Extract the i-th value from each label list\n",
        "    label_values = []\n",
        "    for idx in range(len(test_df_copy)):\n",
        "        labels_raw = test_df_copy.iloc[idx]['labels']\n",
        "\n",
        "        # Parse if string\n",
        "        if isinstance(labels_raw, str):\n",
        "            labels = ast.literal_eval(labels_raw)\n",
        "        else:\n",
        "            labels = labels_raw\n",
        "\n",
        "        label_values.append(labels[i])\n",
        "\n",
        "    # Count values\n",
        "    value_counts = pd.Series(label_values).value_counts().sort_index()\n",
        "    null_count = pd.Series(label_values).isna().sum()\n",
        "    total = len(label_values)\n",
        "\n",
        "    for value, count in value_counts.items():\n",
        "        pct = (count/total)*100\n",
        "        print(f\"  {value:>3}: {count:>5} ({pct:>5.1f}%)\")\n",
        "    if null_count > 0:\n",
        "        pct = (null_count/total)*100\n",
        "        print(f\"  Null: {null_count:>5} ({pct:>5.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DN10KKyytoo",
        "outputId": "b60c83a1-902a-4269-b307-c7adbef2052b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "LABEL DISTRIBUTION IN TEST SET\n",
            "======================================================================\n",
            "\n",
            "LA_cavity:\n",
            "  -50:    91 (  1.4%)\n",
            "   -3:    10 (  0.2%)\n",
            "   -2:   486 (  7.4%)\n",
            "    0:  4379 ( 66.3%)\n",
            "    1:  1076 ( 16.3%)\n",
            "    2:   565 (  8.6%)\n",
            "    3:     1 (  0.0%)\n",
            "\n",
            "RA_dilated:\n",
            "    0:  4563 ( 69.1%)\n",
            "    1:  2045 ( 30.9%)\n",
            "\n",
            "LV_systolic:\n",
            "  -50:    30 (  0.5%)\n",
            "   -3:    38 (  0.6%)\n",
            "   -2:   123 (  1.9%)\n",
            "   -1:   262 (  4.0%)\n",
            "    0:  4719 ( 71.4%)\n",
            "    1:   452 (  6.8%)\n",
            "    2:   385 (  5.8%)\n",
            "    3:   599 (  9.1%)\n",
            "\n",
            "LV_cavity:\n",
            "  -50:     8 (  0.1%)\n",
            "   -3:    28 (  0.4%)\n",
            "   -2:    31 (  0.5%)\n",
            "   -1:   138 (  2.1%)\n",
            "    0:  5806 ( 87.9%)\n",
            "    1:   232 (  3.5%)\n",
            "    2:   292 (  4.4%)\n",
            "    3:    73 (  1.1%)\n",
            "\n",
            "LV_wall:\n",
            "  -50:    11 (  0.2%)\n",
            "   -3:    26 (  0.4%)\n",
            "   -2:    30 (  0.5%)\n",
            "    0:  4418 ( 66.9%)\n",
            "    1:  1768 ( 26.8%)\n",
            "    2:   280 (  4.2%)\n",
            "    3:    75 (  1.1%)\n",
            "\n",
            "RV_cavity:\n",
            "  -50:    41 (  0.6%)\n",
            "   -3:   189 (  2.9%)\n",
            "   -2:   311 (  4.7%)\n",
            "   -1:    23 (  0.3%)\n",
            "    0:  5240 ( 79.3%)\n",
            "    1:   491 (  7.4%)\n",
            "    2:   313 (  4.7%)\n",
            "\n",
            "RV_systolic:\n",
            "  -50:    48 (  0.7%)\n",
            "   -3:   212 (  3.2%)\n",
            "   -2:   375 (  5.7%)\n",
            "    0:  5266 ( 79.7%)\n",
            "    1:   341 (  5.2%)\n",
            "    2:   199 (  3.0%)\n",
            "    3:   167 (  2.5%)\n",
            "\n",
            "AV_stenosis:\n",
            "  -50:     2 (  0.0%)\n",
            "   -3:    20 (  0.3%)\n",
            "   -2:    30 (  0.5%)\n",
            "    0:  5792 ( 87.7%)\n",
            "    1:   339 (  5.1%)\n",
            "    2:   129 (  2.0%)\n",
            "    3:   296 (  4.5%)\n",
            "\n",
            "MV_stenosis:\n",
            "  -50:     2 (  0.0%)\n",
            "   -3:     7 (  0.1%)\n",
            "    0:  6510 ( 98.5%)\n",
            "    1:    61 (  0.9%)\n",
            "    2:    20 (  0.3%)\n",
            "    3:     8 (  0.1%)\n",
            "\n",
            "TV_regurgitation:\n",
            "  -50:    64 (  1.0%)\n",
            "   -3:   171 (  2.6%)\n",
            "   -2:    54 (  0.8%)\n",
            "    0:  6316 ( 95.6%)\n",
            "    2:     2 (  0.0%)\n",
            "    3:     1 (  0.0%)\n",
            "\n",
            "TV_stenosis:\n",
            "  -50:     7 (  0.1%)\n",
            "   -3:   226 (  3.4%)\n",
            "    0:  6373 ( 96.4%)\n",
            "    1:     2 (  0.0%)\n",
            "\n",
            "TV_pulm_htn:\n",
            "  -50:    59 (  0.9%)\n",
            "   -3:   781 ( 11.8%)\n",
            "    0:  3377 ( 51.1%)\n",
            "    1:  1312 ( 19.9%)\n",
            "    2:   947 ( 14.3%)\n",
            "    3:   132 (  2.0%)\n",
            "\n",
            "AV_regurgitation:\n",
            "  -50:    25 (  0.4%)\n",
            "   -3:     1 (  0.0%)\n",
            "   -2:    35 (  0.5%)\n",
            "    0:  4461 ( 67.5%)\n",
            "    1:  1904 ( 28.8%)\n",
            "    2:   115 (  1.7%)\n",
            "    3:    67 (  1.0%)\n",
            "\n",
            "MV_regurgitation:\n",
            "  -50:   170 (  2.6%)\n",
            "   -2:    79 (  1.2%)\n",
            "    0:  3894 ( 58.9%)\n",
            "    1:  1666 ( 25.2%)\n",
            "    2:   497 (  7.5%)\n",
            "    3:   302 (  4.6%)\n",
            "\n",
            "RA_pressure:\n",
            "   -3:    73 (  1.1%)\n",
            "    0:  6251 ( 94.6%)\n",
            "    1:   207 (  3.1%)\n",
            "    2:    77 (  1.2%)\n",
            "\n",
            "LV_diastolic:\n",
            "  -50:     7 (  0.1%)\n",
            "   -3:    61 (  0.9%)\n",
            "   -2:   263 (  4.0%)\n",
            "    0:  6163 ( 93.3%)\n",
            "    1:    61 (  0.9%)\n",
            "    2:    31 (  0.5%)\n",
            "    3:    22 (  0.3%)\n",
            "\n",
            "RV_volume_overload:\n",
            "  -50:     1 (  0.0%)\n",
            "   -3:   221 (  3.3%)\n",
            "    0:  6201 ( 93.8%)\n",
            "    1:   185 (  2.8%)\n",
            "\n",
            "RV_wall:\n",
            "  -50:     4 (  0.1%)\n",
            "   -3:   221 (  3.3%)\n",
            "    0:  6220 ( 94.1%)\n",
            "    1:   163 (  2.5%)\n",
            "\n",
            "RV_pressure_overload:\n",
            "  -50:     1 (  0.0%)\n",
            "   -3:   217 (  3.3%)\n",
            "    0:  6168 ( 93.3%)\n",
            "    1:   222 (  3.4%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# PART 2: CALCULATE ACCURACY\n",
        "# ==============================================================================\n",
        "\n",
        "# Parse all predictions\n",
        "print(\"\\n\\nParsing predictions...\")\n",
        "results_df['pred_labels'] = results_df['prediction_text'].apply(parse_prediction)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ACCURACY BY LABEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "accuracy_results = []\n",
        "\n",
        "for i, label_name in enumerate(LABEL_NAMES):\n",
        "    # Extract true values (i-th element from true_labels list)\n",
        "    true_vals = results_df['true_labels'].apply(lambda x: x[i] if i < len(x) else None).values\n",
        "\n",
        "    # Extract predicted values\n",
        "    pred_vals = results_df['pred_labels'].apply(lambda x: x[i] if x and i < len(x) else None).values\n",
        "\n",
        "    # Remove any None predictions\n",
        "    valid_mask = ~pd.isna(pred_vals)\n",
        "    true_vals_valid = true_vals[valid_mask]\n",
        "    pred_vals_valid = pred_vals[valid_mask]\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct = (true_vals_valid == pred_vals_valid).sum()\n",
        "    total = len(true_vals_valid)\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "    # Count unparseable predictions\n",
        "    unparseable = (~valid_mask).sum()\n",
        "\n",
        "    accuracy_results.append({\n",
        "        'label': label_name,\n",
        "        'correct': correct,\n",
        "        'total': total,\n",
        "        'accuracy': accuracy,\n",
        "        'unparseable': unparseable\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{label_name}:\")\n",
        "    print(f\"  Correct: {correct}/{total} = {accuracy:.3f}\")\n",
        "    if unparseable > 0:\n",
        "        print(f\"  Unparseable: {unparseable}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um2SYHO8ywQf",
        "outputId": "f3cc5d89-3c04-4b17-e7d2-f434c762f7ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Parsing predictions...\n",
            "\n",
            "======================================================================\n",
            "ACCURACY BY LABEL\n",
            "======================================================================\n",
            "\n",
            "LA_cavity:\n",
            "  Correct: 6594/6605 = 0.998\n",
            "  Unparseable: 3\n",
            "\n",
            "RA_dilated:\n",
            "  Correct: 6605/6605 = 1.000\n",
            "  Unparseable: 3\n",
            "\n",
            "LV_systolic:\n",
            "  Correct: 6596/6605 = 0.999\n",
            "  Unparseable: 3\n",
            "\n",
            "LV_cavity:\n",
            "  Correct: 6602/6605 = 1.000\n",
            "  Unparseable: 3\n",
            "\n",
            "LV_wall:\n",
            "  Correct: 6595/6605 = 0.998\n",
            "  Unparseable: 3\n",
            "\n",
            "RV_cavity:\n",
            "  Correct: 6603/6605 = 1.000\n",
            "  Unparseable: 3\n",
            "\n",
            "RV_systolic:\n",
            "  Correct: 6599/6605 = 0.999\n",
            "  Unparseable: 3\n",
            "\n",
            "AV_stenosis:\n",
            "  Correct: 6596/6605 = 0.999\n",
            "  Unparseable: 3\n",
            "\n",
            "MV_stenosis:\n",
            "  Correct: 6600/6605 = 0.999\n",
            "  Unparseable: 3\n",
            "\n",
            "TV_regurgitation:\n",
            "  Correct: 6600/6605 = 0.999\n",
            "  Unparseable: 3\n",
            "\n",
            "TV_stenosis:\n",
            "  Correct: 6604/6605 = 1.000\n",
            "  Unparseable: 3\n",
            "\n",
            "TV_pulm_htn:\n",
            "  Correct: 6601/6605 = 0.999\n",
            "  Unparseable: 3\n",
            "\n",
            "AV_regurgitation:\n",
            "  Correct: 6577/6605 = 0.996\n",
            "  Unparseable: 3\n",
            "\n",
            "MV_regurgitation:\n",
            "  Correct: 6579/6605 = 0.996\n",
            "  Unparseable: 3\n",
            "\n",
            "RA_pressure:\n",
            "  Correct: 6605/6605 = 1.000\n",
            "  Unparseable: 3\n",
            "\n",
            "LV_diastolic:\n",
            "  Correct: 6596/6605 = 0.999\n",
            "  Unparseable: 3\n",
            "\n",
            "RV_volume_overload:\n",
            "  Correct: 6603/6605 = 1.000\n",
            "  Unparseable: 3\n",
            "\n",
            "RV_wall:\n",
            "  Correct: 6604/6605 = 1.000\n",
            "  Unparseable: 3\n",
            "\n",
            "RV_pressure_overload:\n",
            "  Correct: 6602/6605 = 1.000\n",
            "  Unparseable: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create accuracy summary DataFrame\n",
        "accuracy_df = pd.DataFrame(accuracy_results)\n",
        "\n",
        "# Save accuracy results\n",
        "accuracy_df.to_csv('label_accuracy.csv', index=False)\n",
        "print(\"\\nAccuracy results saved to label_accuracy.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ns08RtFyymJ",
        "outputId": "b3fac9e2-d434-434d-d276-e23f18f92964"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "EXACT MATCH (all 19 labels correct): 6478/6608 = 0.980\n",
            "======================================================================\n",
            "\n",
            "Accuracy results saved to label_accuracy.csv\n",
            "\n",
            "ACCURACY SUMMARY:\n",
            "               label  correct  total  accuracy  unparseable\n",
            "           LA_cavity     6594   6605  0.998335            3\n",
            "          RA_dilated     6605   6605  1.000000            3\n",
            "         LV_systolic     6596   6605  0.998637            3\n",
            "           LV_cavity     6602   6605  0.999546            3\n",
            "             LV_wall     6595   6605  0.998486            3\n",
            "           RV_cavity     6603   6605  0.999697            3\n",
            "         RV_systolic     6599   6605  0.999092            3\n",
            "         AV_stenosis     6596   6605  0.998637            3\n",
            "         MV_stenosis     6600   6605  0.999243            3\n",
            "    TV_regurgitation     6600   6605  0.999243            3\n",
            "         TV_stenosis     6604   6605  0.999849            3\n",
            "         TV_pulm_htn     6601   6605  0.999394            3\n",
            "    AV_regurgitation     6577   6605  0.995761            3\n",
            "    MV_regurgitation     6579   6605  0.996064            3\n",
            "         RA_pressure     6605   6605  1.000000            3\n",
            "        LV_diastolic     6596   6605  0.998637            3\n",
            "  RV_volume_overload     6603   6605  0.999697            3\n",
            "             RV_wall     6604   6605  0.999849            3\n",
            "RV_pressure_overload     6602   6605  0.999546            3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter accuracy results to include only values between -1 and 3\n",
        "filtered_accuracy_df = accuracy_df[accuracy_df['label'].apply(lambda x: x in ['LV_systolic', 'LV_cavity', 'LV_wall', 'RV_cavity', 'RV_systolic', 'AV_stenosis', 'MV_stenosis', 'TV_regurgitation', 'TV_stenosis', 'TV_pulm_htn', 'AV_regurgitation', 'MV_regurgitation', 'RA_pressure', 'LV_diastolic', 'RV_volume_overload', 'RV_wall', 'RV_pressure_overload'])]\n",
        "\n",
        "\n",
        "# Display filtered summary\n",
        "print(\"\\nFILTERED ACCURACY SUMMARY (values -1 to 3):\")\n",
        "print(filtered_accuracy_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "dyJB3lqjpcgg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
